{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Sensitivity Analysis\n",
    "\n",
    "This notebook performs a comprehensive sensitivity analysis comparing results with and without outliers across different detection methods.\n",
    "\n",
    "## Objectives\n",
    "- Compare statistical measures before and after outlier removal\n",
    "- Evaluate impact on model performance\n",
    "- Visualize differences in data distributions\n",
    "- Provide recommendations for outlier handling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "import outliers\n",
    "import utils\n",
    "import cleaning\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìä Outlier Sensitivity Analysis Setup Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample bike demand data\n",
    "bike_data = pd.read_csv('../data/sample-data.csv')\n",
    "\n",
    "print(\"üìà Bike Demand Dataset:\")\n",
    "print(f\"Shape: {bike_data.shape}\")\n",
    "print(f\"Columns: {bike_data.columns.tolist()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(bike_data.head())\n",
    "\n",
    "print(\"\\nüìä Basic Statistics:\")\n",
    "print(bike_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Outlier Detection Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to analyze\n",
    "numeric_columns = ['hour', 'temperature', 'humidity', 'demand']\n",
    "target_column = 'demand'\n",
    "feature_columns = ['hour', 'temperature', 'humidity']\n",
    "\n",
    "# Generate outlier summary for different methods\n",
    "outlier_methods = ['iqr', 'zscore', 'modified_zscore']\n",
    "summary_df = outliers.outlier_summary(bike_data, numeric_columns, outlier_methods)\n",
    "\n",
    "print(\"üîç Outlier Detection Summary:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Create visualization of outlier counts\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Outlier counts by method\n",
    "summary_pivot = summary_df.pivot(index='Column', columns='Method', values='Outlier_Count')\n",
    "summary_pivot.plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Outlier Count by Detection Method')\n",
    "axes[0].set_ylabel('Number of Outliers')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].legend(title='Method')\n",
    "\n",
    "# Outlier percentages by method\n",
    "summary_pivot_pct = summary_df.pivot(index='Column', columns='Method', values='Outlier_Percentage')\n",
    "summary_pivot_pct.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Outlier Percentage by Detection Method')\n",
    "axes[1].set_ylabel('Percentage of Outliers')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].legend(title='Method')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Outlier detection summary complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visual Comparison: Before and After Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visual comparison\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "fig.suptitle('Data Distribution: Original vs Outlier Removal Methods', fontsize=16, fontweight='bold')\n",
    "\n",
    "methods_to_compare = ['original'] + [f'{method}_removed' for method in outlier_methods]\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "for i, column in enumerate(['temperature', 'humidity', 'demand']):\n",
    "    # Box plots\n",
    "    box_data = []\n",
    "    box_labels = []\n",
    "    \n",
    "    original_data = bike_data[column]\n",
    "    box_data.append(original_data)\n",
    "    box_labels.append('Original')\n",
    "    \n",
    "    for method in outlier_methods:\n",
    "        cleaned_data = outliers.remove_outliers(bike_data, [column], method=method)\n",
    "        box_data.append(cleaned_data[column])\n",
    "        box_labels.append(f'{method.upper()}')\n",
    "    \n",
    "    axes[i, 0].boxplot(box_data, labels=box_labels)\n",
    "    axes[i, 0].set_title(f'{column.title()} - Box Plot Comparison')\n",
    "    axes[i, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Histograms\n",
    "    axes[i, 1].hist(original_data, bins=20, alpha=0.7, color='blue', label='Original')\n",
    "    axes[i, 1].set_title(f'{column.title()} - Original Distribution')\n",
    "    axes[i, 1].set_xlabel(column.title())\n",
    "    axes[i, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # IQR method comparison\n",
    "    iqr_cleaned = outliers.remove_outliers(bike_data, [column], method='iqr')\n",
    "    axes[i, 2].hist(iqr_cleaned[column], bins=20, alpha=0.7, color='red', label='IQR Cleaned')\n",
    "    axes[i, 2].set_title(f'{column.title()} - IQR Method')\n",
    "    axes[i, 2].set_xlabel(column.title())\n",
    "    axes[i, 2].set_ylabel('Frequency')\n",
    "    \n",
    "    # Z-score method comparison\n",
    "    zscore_cleaned = outliers.remove_outliers(bike_data, [column], method='zscore')\n",
    "    axes[i, 3].hist(zscore_cleaned[column], bins=20, alpha=0.7, color='green', label='Z-score Cleaned')\n",
    "    axes[i, 3].set_title(f'{column.title()} - Z-score Method')\n",
    "    axes[i, 3].set_xlabel(column.title())\n",
    "    axes[i, 3].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Visual comparison complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comprehensive sensitivity analysis\n",
    "def custom_analysis(df):\n",
    "    \"\"\"Custom analysis function for sensitivity testing\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Correlation analysis\n",
    "    corr_matrix = df[numeric_columns].corr()\n",
    "    results['demand_correlations'] = corr_matrix['demand'].to_dict()\n",
    "    \n",
    "    # Simple linear regression: temperature vs demand\n",
    "    if len(df) > 5:  # Ensure we have enough data\n",
    "        X = df[['temperature']].values\n",
    "        y = df['demand'].values\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        results['regression_r2'] = r2_score(y, y_pred)\n",
    "        results['regression_mse'] = mean_squared_error(y, y_pred)\n",
    "        results['regression_coef'] = model.coef_[0]\n",
    "        results['regression_intercept'] = model.intercept_\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run sensitivity analysis\n",
    "sensitivity_results = outliers.sensitivity_analysis(\n",
    "    bike_data,\n",
    "    target_column='demand',\n",
    "    feature_columns=feature_columns,\n",
    "    outlier_methods=outlier_methods,\n",
    "    analysis_func=custom_analysis\n",
    ")\n",
    "\n",
    "print(\"üî¨ Sensitivity Analysis Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for method_name, results in sensitivity_results.items():\n",
    "    row = {\n",
    "        'Method': method_name,\n",
    "        'Data_Points': results['data_shape'][0],\n",
    "        'Demand_Mean': results['target_stats']['mean'],\n",
    "        'Demand_Std': results['target_stats']['std'],\n",
    "        'Demand_Skew': results['target_stats'].get('skew', 'N/A'),\n",
    "        'Temp_Correlation': results.get('custom_analysis', {}).get('demand_correlations', {}).get('temperature', 'N/A'),\n",
    "        'Regression_R2': results.get('custom_analysis', {}).get('regression_r2', 'N/A'),\n",
    "        'Regression_Coef': results.get('custom_analysis', {}).get('regression_coef', 'N/A')\n",
    "    }\n",
    "    \n",
    "    if 'rows_removed' in results:\n",
    "        row['Rows_Removed'] = results['rows_removed']\n",
    "        row['Removal_Pct'] = results['removal_percentage']\n",
    "    else:\n",
    "        row['Rows_Removed'] = 0\n",
    "        row['Removal_Pct'] = 0.0\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nüìä Comparison Table:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Save comparison results\n",
    "comparison_path = utils.save_with_timestamp(\n",
    "    df=comparison_df,\n",
    "    prefix=\"outlier_sensitivity_comparison\",\n",
    "    source=\"analysis\",\n",
    "    ext=\"csv\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüíæ Comparison results saved to: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Performance Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance with and without outliers\n",
    "def evaluate_model_performance(df, model_type='linear'):\n",
    "    \"\"\"Evaluate model performance on given dataset\"\"\"\n",
    "    if len(df) < 10:  # Need minimum data for meaningful evaluation\n",
    "        return {'error': 'Insufficient data'}\n",
    "    \n",
    "    X = df[feature_columns]\n",
    "    y = df[target_column]\n",
    "    \n",
    "    if model_type == 'linear':\n",
    "        model = LinearRegression()\n",
    "    else:\n",
    "        model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    return {\n",
    "        'r2_score': r2_score(y, y_pred),\n",
    "        'mse': mean_squared_error(y, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y, y_pred)),\n",
    "        'mean_actual': y.mean(),\n",
    "        'std_actual': y.std()\n",
    "    }\n",
    "\n",
    "# Test different models\n",
    "model_results = {}\n",
    "\n",
    "# Original data\n",
    "model_results['original_linear'] = evaluate_model_performance(bike_data, 'linear')\n",
    "model_results['original_rf'] = evaluate_model_performance(bike_data, 'rf')\n",
    "\n",
    "# Test each outlier removal method\n",
    "for method in outlier_methods:\n",
    "    cleaned_data = outliers.remove_outliers(bike_data, numeric_columns, method=method)\n",
    "    model_results[f'{method}_linear'] = evaluate_model_performance(cleaned_data, 'linear')\n",
    "    model_results[f'{method}_rf'] = evaluate_model_performance(cleaned_data, 'rf')\n",
    "\n",
    "# Create model performance comparison\n",
    "model_comparison = []\n",
    "for key, results in model_results.items():\n",
    "    if 'error' not in results:\n",
    "        method, model_type = key.rsplit('_', 1)\n",
    "        model_comparison.append({\n",
    "            'Method': method,\n",
    "            'Model': model_type,\n",
    "            'R2_Score': results['r2_score'],\n",
    "            'RMSE': results['rmse'],\n",
    "            'Mean_Actual': results['mean_actual'],\n",
    "            'Std_Actual': results['std_actual']\n",
    "        })\n",
    "\n",
    "model_comparison_df = pd.DataFrame(model_comparison)\n",
    "\n",
    "print(\"ü§ñ Model Performance Comparison:\")\n",
    "print(model_comparison_df.round(4))\n",
    "\n",
    "# Visualize model performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# R¬≤ Score comparison\n",
    "pivot_r2 = model_comparison_df.pivot(index='Method', columns='Model', values='R2_Score')\n",
    "pivot_r2.plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Model R¬≤ Score Comparison')\n",
    "axes[0].set_ylabel('R¬≤ Score')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].legend(title='Model Type')\n",
    "\n",
    "# RMSE comparison\n",
    "pivot_rmse = model_comparison_df.pivot(index='Method', columns='Model', values='RMSE')\n",
    "pivot_rmse.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Model RMSE Comparison')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].legend(title='Model Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Model performance analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regression Fit Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression fit comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Regression Fit Comparison: Temperature vs Demand', fontsize=16, fontweight='bold')\n",
    "\n",
    "datasets = {\n",
    "    'Original': bike_data,\n",
    "    'IQR Cleaned': outliers.remove_outliers(bike_data, numeric_columns, method='iqr'),\n",
    "    'Z-Score Cleaned': outliers.remove_outliers(bike_data, numeric_columns, method='zscore'),\n",
    "    'Modified Z-Score Cleaned': outliers.remove_outliers(bike_data, numeric_columns, method='modified_zscore')\n",
    "}\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "positions = [(0,0), (0,1), (1,0), (1,1)]\n",
    "\n",
    "for i, (name, data) in enumerate(datasets.items()):\n",
    "    row, col = positions[i]\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[row, col].scatter(data['temperature'], data['demand'], alpha=0.6, color=colors[i])\n",
    "    \n",
    "    # Fit regression line\n",
    "    if len(data) > 5:\n",
    "        X = data[['temperature']].values\n",
    "        y = data['demand'].values\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Plot regression line\n",
    "        x_range = np.linspace(data['temperature'].min(), data['temperature'].max(), 100)\n",
    "        y_pred = model.predict(x_range.reshape(-1, 1))\n",
    "        axes[row, col].plot(x_range, y_pred, color='black', linewidth=2, linestyle='--')\n",
    "        \n",
    "        # Calculate R¬≤\n",
    "        y_pred_all = model.predict(X)\n",
    "        r2 = r2_score(y, y_pred_all)\n",
    "        \n",
    "        axes[row, col].set_title(f'{name}\\nR¬≤ = {r2:.4f}, N = {len(data)}')\n",
    "    else:\n",
    "        axes[row, col].set_title(f'{name}\\nInsufficient data')\n",
    "    \n",
    "    axes[row, col].set_xlabel('Temperature')\n",
    "    axes[row, col].set_ylabel('Demand')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Regression fit comparison complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recommendations and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on analysis\n",
    "print(\"üéØ OUTLIER ANALYSIS RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze results to provide recommendations\n",
    "original_r2 = model_results.get('original_linear', {}).get('r2_score', 0)\n",
    "best_method = None\n",
    "best_r2 = original_r2\n",
    "\n",
    "for method in outlier_methods:\n",
    "    method_r2 = model_results.get(f'{method}_linear', {}).get('r2_score', 0)\n",
    "    if method_r2 > best_r2:\n",
    "        best_r2 = method_r2\n",
    "        best_method = method\n",
    "\n",
    "print(f\"\\nüìä ANALYSIS SUMMARY:\")\n",
    "print(f\"Original dataset R¬≤ score: {original_r2:.4f}\")\n",
    "if best_method:\n",
    "    print(f\"Best performing method: {best_method.upper()} (R¬≤ = {best_r2:.4f})\")\n",
    "    improvement = ((best_r2 - original_r2) / original_r2) * 100\n",
    "    print(f\"Performance improvement: {improvement:.2f}%\")\n",
    "else:\n",
    "    print(\"No outlier removal method improved model performance\")\n",
    "\n",
    "print(f\"\\nüîç OUTLIER DETECTION INSIGHTS:\")\n",
    "for method in outlier_methods:\n",
    "    method_summary = summary_df[summary_df['Method'] == method]\n",
    "    total_outliers = method_summary['Outlier_Count'].sum()\n",
    "    avg_percentage = method_summary['Outlier_Percentage'].mean()\n",
    "    print(f\"- {method.upper()}: {total_outliers} total outliers ({avg_percentage:.2f}% average)\")\n",
    "\n",
    "print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "if best_method:\n",
    "    print(f\"1. Use {best_method.upper()} method for outlier detection in this dataset\")\n",
    "    print(f\"2. This method provides the best balance of outlier removal and model performance\")\nelse:\n",
    "    print(\"1. Consider keeping outliers as they may contain valuable information\")\n",
    "    print(\"2. Outlier removal did not improve model performance for this dataset\")\n",
    "\n",
    "print(f\"3. Monitor outliers regularly as they may indicate data quality issues\")\n",
    "print(f\"4. Consider domain expertise when deciding on outlier treatment\")\n",
    "print(f\"5. Document outlier assumptions and treatment decisions for reproducibility\")\n",
    "\n",
    "print(f\"\\nüìã NEXT STEPS:\")\n",
    "print(f\"- Implement chosen outlier detection method in data preprocessing pipeline\")\n",
    "print(f\"- Set up automated outlier monitoring and alerting\")\n",
    "print(f\"- Document outlier handling decisions in project documentation\")\n",
    "print(f\"- Consider ensemble methods that are robust to outliers\")\n",
    "\n",
    "# Save final summary\n",
    "summary_results = {\n",
    "    'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset_shape': bike_data.shape,\n",
    "    'methods_tested': outlier_methods,\n",
    "    'original_r2': original_r2,\n",
    "    'best_method': best_method,\n",
    "    'best_r2': best_r2,\n",
    "    'improvement_percentage': ((best_r2 - original_r2) / original_r2) * 100 if best_method else 0\n",
    "}\n",
    "\n",
    "summary_df_final = pd.DataFrame([summary_results])\n",
    "summary_path = utils.save_with_timestamp(\n",
    "    df=summary_df_final,\n",
    "    prefix=\"outlier_analysis_summary\",\n",
    "    source=\"analysis\",\n",
    "    ext=\"csv\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüíæ Analysis summary saved to: {summary_path}\")\n",
    "print(\"\\n‚úÖ Outlier sensitivity analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
