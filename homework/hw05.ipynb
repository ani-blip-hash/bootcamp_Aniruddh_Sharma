{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa5d355-1452-491a-b658-2d109604b180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Data Storage Assignment - Implementation\n",
      "==================================================\n",
      "\n",
      "📁 TASK 1: Save DataFrame in Two Formats\n",
      "----------------------------------------\n",
      "📂 Raw data directory: data/raw\n",
      "📂 Processed data directory: data/processed\n",
      "\n",
      "📊 Creating sample dataset...\n",
      "✅ Created dataset with shape: (365, 13)\n",
      "📅 Date range: 2023-01-01 00:00:00 to 2023-12-31 00:00:00\n",
      "🏷️  Columns: ['date', 'symbol', 'open', 'high', 'low', 'close', 'volume', 'data_source', 'fetch_timestamp', 'daily_return', 'volatility_20d', 'sma_20', 'sma_50']\n",
      "💾 Saved raw data as CSV: data/raw\\stock_data_raw.csv\n",
      "💾 Saved processed data as Parquet: data/processed\\stock_data_processed.parquet\n",
      "\n",
      "🔄 TASK 2: Reload and Validate Data\n",
      "----------------------------------------\n",
      "📥 Reloading CSV file...\n",
      "✅ CSV reloaded successfully: (365, 13)\n",
      "\n",
      "📥 Reloading Parquet file...\n",
      "✅ Parquet reloaded successfully: (365, 13)\n",
      "\n",
      "🔍 Validating CSV...\n",
      "✅ Shape matches: (365, 13)\n",
      "✅ All 13 columns present\n",
      "✅ Critical dtypes preserved\n",
      "✅ Data integrity verified\n",
      "\n",
      "📋 Validation Summary for CSV: ✅ PASSED\n",
      "\n",
      "🔍 Validating Parquet...\n",
      "✅ Shape matches: (365, 13)\n",
      "✅ All 13 columns present\n",
      "⚠️  Dtype issues found:\n",
      "   volume: expected int64, got int32\n",
      "✅ Data integrity verified\n",
      "\n",
      "📋 Validation Summary for Parquet: ❌ FAILED\n",
      "\n",
      "🔧 TASK 3: Utility Functions\n",
      "----------------------------------------\n",
      "\n",
      "🧪 Testing utility functions...\n",
      "💾 Written CSV: data/raw\\test_utilities.csv (10 rows)\n",
      "💾 Written Parquet: data/processed\\test_utilities.parquet (10 rows)\n",
      "📥 Read CSV: data/raw\\test_utilities.csv (10 rows)\n",
      "📥 Read Parquet: data/processed\\test_utilities.parquet (10 rows)\n",
      "✅ Utility functions work correctly!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"🚀 Data Storage Assignment - Implementation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# =============================================================================\n",
    "# TASK 1: SAVE IN TWO FORMATS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n📁 TASK 1: Save DataFrame in Two Formats\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# TODO: Load environment variables for data directories\n",
    "DATA_DIR_RAW = os.getenv(\"DATA_DIR_RAW\", \"data/raw\")\n",
    "DATA_DIR_PROCESSED = os.getenv(\"DATA_DIR_PROCESSED\", \"data/processed\")\n",
    "\n",
    "print(f\"📂 Raw data directory: {DATA_DIR_RAW}\")\n",
    "print(f\"📂 Processed data directory: {DATA_DIR_PROCESSED}\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(DATA_DIR_RAW, exist_ok=True)\n",
    "os.makedirs(DATA_DIR_PROCESSED, exist_ok=True)\n",
    "\n",
    "# Create sample dataset using our enhanced stock data fetching\n",
    "print(\"\\n📊 Creating sample dataset...\")\n",
    "\n",
    "# Generate sample stock data (since we may not have API keys in all environments)\n",
    "np.random.seed(42)  # For reproducible data\n",
    "dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
    "n_days = len(dates)\n",
    "\n",
    "# Simulate realistic stock data for MSFT\n",
    "base_price = 250.0\n",
    "daily_returns = np.random.normal(0.0005, 0.02, n_days)  # Small daily returns with volatility\n",
    "prices = [base_price]\n",
    "\n",
    "for return_rate in daily_returns[1:]:\n",
    "    prices.append(prices[-1] * (1 + return_rate))\n",
    "\n",
    "# Create comprehensive DataFrame\n",
    "sample_data = {\n",
    "    'date': dates,\n",
    "    'symbol': 'MSFT',\n",
    "    'open': [p * np.random.uniform(0.995, 1.005) for p in prices],\n",
    "    'high': [p * np.random.uniform(1.01, 1.03) for p in prices],\n",
    "    'low': [p * np.random.uniform(0.97, 0.99) for p in prices],\n",
    "    'close': prices,\n",
    "    'volume': np.random.randint(20000000, 100000000, n_days),\n",
    "    'data_source': 'simulated',\n",
    "    'fetch_timestamp': datetime.now()\n",
    "}\n",
    "\n",
    "df_stock = pd.DataFrame(sample_data)\n",
    "\n",
    "# Add some calculated fields for processing\n",
    "df_stock['daily_return'] = df_stock['close'].pct_change()\n",
    "df_stock['volatility_20d'] = df_stock['daily_return'].rolling(20).std()\n",
    "df_stock['sma_20'] = df_stock['close'].rolling(20).mean()\n",
    "df_stock['sma_50'] = df_stock['close'].rolling(50).mean()\n",
    "\n",
    "print(f\"✅ Created dataset with shape: {df_stock.shape}\")\n",
    "print(f\"📅 Date range: {df_stock['date'].min()} to {df_stock['date'].max()}\")\n",
    "print(f\"🏷️  Columns: {list(df_stock.columns)}\")\n",
    "\n",
    "# TODO: Save to data/raw/ as CSV\n",
    "csv_path = os.path.join(DATA_DIR_RAW, \"stock_data_raw.csv\")\n",
    "df_stock.to_csv(csv_path, index=False)\n",
    "print(f\"💾 Saved raw data as CSV: {csv_path}\")\n",
    "\n",
    "# TODO: Save to data/processed/ as Parquet\n",
    "parquet_path = os.path.join(DATA_DIR_PROCESSED, \"stock_data_processed.parquet\")\n",
    "try:\n",
    "    df_stock.to_parquet(parquet_path, index=False, engine='pyarrow')\n",
    "    print(f\"💾 Saved processed data as Parquet: {parquet_path}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Parquet save failed - missing engine: {e}\")\n",
    "    print(\"💡 Install with: pip install pyarrow\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Parquet save failed: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TASK 2: RELOAD AND VALIDATE\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n🔄 TASK 2: Reload and Validate Data\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def validate_dataframe(df_original, df_reloaded, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Validate that reloaded DataFrame matches original specifications.\n",
    "    \n",
    "    Args:\n",
    "        df_original: Original DataFrame\n",
    "        df_reloaded: Reloaded DataFrame to validate\n",
    "        name: Name for reporting\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if validation passes\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 Validating {name}...\")\n",
    "    \n",
    "    validation_results = {\n",
    "        'shape_match': False,\n",
    "        'columns_match': False,\n",
    "        'dtypes_preserved': False,\n",
    "        'data_integrity': False\n",
    "    }\n",
    "    \n",
    "    # Check shape\n",
    "    if df_original.shape == df_reloaded.shape:\n",
    "        validation_results['shape_match'] = True\n",
    "        print(f\"✅ Shape matches: {df_reloaded.shape}\")\n",
    "    else:\n",
    "        print(f\"❌ Shape mismatch: Original {df_original.shape} vs Reloaded {df_reloaded.shape}\")\n",
    "        return False\n",
    "    \n",
    "    # Check columns\n",
    "    if list(df_original.columns) == list(df_reloaded.columns):\n",
    "        validation_results['columns_match'] = True\n",
    "        print(f\"✅ All {len(df_reloaded.columns)} columns present\")\n",
    "    else:\n",
    "        missing_cols = set(df_original.columns) - set(df_reloaded.columns)\n",
    "        extra_cols = set(df_reloaded.columns) - set(df_original.columns)\n",
    "        print(f\"❌ Column mismatch:\")\n",
    "        if missing_cols:\n",
    "            print(f\"   Missing: {missing_cols}\")\n",
    "        if extra_cols:\n",
    "            print(f\"   Extra: {extra_cols}\")\n",
    "        return False\n",
    "    \n",
    "    # Check critical dtypes\n",
    "    critical_columns = {\n",
    "        'date': 'datetime64[ns]',\n",
    "        'close': 'float64',\n",
    "        'volume': 'int64',\n",
    "        'symbol': 'object'\n",
    "    }\n",
    "    \n",
    "    dtype_issues = []\n",
    "    for col, expected_dtype in critical_columns.items():\n",
    "        if col in df_reloaded.columns:\n",
    "            actual_dtype = str(df_reloaded[col].dtype)\n",
    "            # Handle datetime comparison (different representations possible)\n",
    "            if 'datetime' in expected_dtype and 'datetime' in actual_dtype:\n",
    "                continue\n",
    "            elif actual_dtype != expected_dtype:\n",
    "                dtype_issues.append(f\"{col}: expected {expected_dtype}, got {actual_dtype}\")\n",
    "    \n",
    "    if not dtype_issues:\n",
    "        validation_results['dtypes_preserved'] = True\n",
    "        print(\"✅ Critical dtypes preserved\")\n",
    "    else:\n",
    "        print(\"⚠️  Dtype issues found:\")\n",
    "        for issue in dtype_issues:\n",
    "            print(f\"   {issue}\")\n",
    "    \n",
    "    # Check data integrity (sample of numeric columns)\n",
    "    numeric_cols = ['open', 'high', 'low', 'close']\n",
    "    integrity_check = True\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df_original.columns and col in df_reloaded.columns:\n",
    "            orig_sum = df_original[col].sum()\n",
    "            reload_sum = df_reloaded[col].sum()\n",
    "            if abs(orig_sum - reload_sum) > 0.01:  # Small tolerance for float precision\n",
    "                print(f\"❌ Data integrity issue in {col}: {orig_sum} vs {reload_sum}\")\n",
    "                integrity_check = False\n",
    "    \n",
    "    if integrity_check:\n",
    "        validation_results['data_integrity'] = True\n",
    "        print(\"✅ Data integrity verified\")\n",
    "    \n",
    "    # Overall result\n",
    "    all_passed = all(validation_results.values())\n",
    "    status = \"✅ PASSED\" if all_passed else \"❌ FAILED\"\n",
    "    print(f\"\\n📋 Validation Summary for {name}: {status}\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "# TODO: Reload CSV file\n",
    "print(\"📥 Reloading CSV file...\")\n",
    "try:\n",
    "    df_csv_reloaded = pd.read_csv(csv_path)\n",
    "    # Convert date column back to datetime (CSV loses this info)\n",
    "    df_csv_reloaded['date'] = pd.to_datetime(df_csv_reloaded['date'])\n",
    "    df_csv_reloaded['fetch_timestamp'] = pd.to_datetime(df_csv_reloaded['fetch_timestamp'])\n",
    "    print(f\"✅ CSV reloaded successfully: {df_csv_reloaded.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to reload CSV: {e}\")\n",
    "    df_csv_reloaded = pd.DataFrame()\n",
    "\n",
    "# TODO: Reload Parquet file  \n",
    "print(\"\\n📥 Reloading Parquet file...\")\n",
    "try:\n",
    "    df_parquet_reloaded = pd.read_parquet(parquet_path, engine='pyarrow')\n",
    "    print(f\"✅ Parquet reloaded successfully: {df_parquet_reloaded.shape}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Parquet reload failed - missing engine: {e}\")\n",
    "    print(\"💡 Install with: pip install pyarrow\")\n",
    "    df_parquet_reloaded = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to reload Parquet: {e}\")\n",
    "    df_parquet_reloaded = pd.DataFrame()\n",
    "\n",
    "# TODO: Validate both reloaded DataFrames\n",
    "if not df_csv_reloaded.empty:\n",
    "    csv_valid = validate_dataframe(df_stock, df_csv_reloaded, \"CSV\")\n",
    "    \n",
    "if not df_parquet_reloaded.empty:\n",
    "    parquet_valid = validate_dataframe(df_stock, df_parquet_reloaded, \"Parquet\")\n",
    "\n",
    "# =============================================================================\n",
    "# TASK 3: REFACTOR TO UTILITIES  \n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n🔧 TASK 3: Utility Functions\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def write_df(df, filepath, **kwargs):\n",
    "    \"\"\"\n",
    "    Write DataFrame to file, routing by file extension.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame to write\n",
    "        filepath: target file path\n",
    "        **kwargs: additional arguments for pandas write methods\n",
    "    \n",
    "    Returns:\n",
    "        str: filepath if successful\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: for unsupported file extensions\n",
    "        ImportError: for missing Parquet engine\n",
    "    \"\"\"\n",
    "    # Ensure directory exists\n",
    "    directory = os.path.dirname(filepath)\n",
    "    if directory:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Get file extension\n",
    "    _, ext = os.path.splitext(filepath)\n",
    "    ext = ext.lower()\n",
    "    \n",
    "    try:\n",
    "        if ext == '.csv':\n",
    "            df.to_csv(filepath, index=False, **kwargs)\n",
    "            print(f\"💾 Written CSV: {filepath} ({len(df)} rows)\")\n",
    "            \n",
    "        elif ext == '.parquet':\n",
    "            try:\n",
    "                # Default to pyarrow, fallback to fastparquet\n",
    "                engine = kwargs.get('engine', 'pyarrow')\n",
    "                df.to_parquet(filepath, index=False, engine=engine, **kwargs)\n",
    "                print(f\"💾 Written Parquet: {filepath} ({len(df)} rows)\")\n",
    "            except ImportError as e:\n",
    "                raise ImportError(\n",
    "                    f\"Missing Parquet engine. Install with: pip install pyarrow\\n\"\n",
    "                    f\"Original error: {e}\"\n",
    "                )\n",
    "                \n",
    "        elif ext in ['.xlsx', '.xls']:\n",
    "            df.to_excel(filepath, index=False, **kwargs)\n",
    "            print(f\"💾 Written Excel: {filepath} ({len(df)} rows)\")\n",
    "            \n",
    "        elif ext == '.json':\n",
    "            df.to_json(filepath, **kwargs)\n",
    "            print(f\"💾 Written JSON: {filepath} ({len(df)} rows)\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "        \n",
    "        return filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to write {filepath}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def read_df(filepath, **kwargs):\n",
    "    \"\"\"\n",
    "    Read DataFrame from file, routing by file extension.\n",
    "    \n",
    "    Args:\n",
    "        filepath: source file path\n",
    "        **kwargs: additional arguments for pandas read methods\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: loaded DataFrame\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: if file doesn't exist\n",
    "        ValueError: for unsupported file extensions\n",
    "        ImportError: for missing Parquet engine\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    # Get file extension\n",
    "    _, ext = os.path.splitext(filepath)\n",
    "    ext = ext.lower()\n",
    "    \n",
    "    try:\n",
    "        if ext == '.csv':\n",
    "            df = pd.read_csv(filepath, **kwargs)\n",
    "            print(f\"📥 Read CSV: {filepath} ({len(df)} rows)\")\n",
    "            \n",
    "        elif ext == '.parquet':\n",
    "            try:\n",
    "                engine = kwargs.get('engine', 'pyarrow')\n",
    "                df = pd.read_parquet(filepath, engine=engine, **kwargs)\n",
    "                print(f\"📥 Read Parquet: {filepath} ({len(df)} rows)\")\n",
    "            except ImportError as e:\n",
    "                raise ImportError(\n",
    "                    f\"Missing Parquet engine. Install with: pip install pyarrow\\n\"\n",
    "                    f\"Original error: {e}\"\n",
    "                )\n",
    "                \n",
    "        elif ext in ['.xlsx', '.xls']:\n",
    "            df = pd.read_excel(filepath, **kwargs)\n",
    "            print(f\"📥 Read Excel: {filepath} ({len(df)} rows)\")\n",
    "            \n",
    "        elif ext == '.json':\n",
    "            df = pd.read_json(filepath, **kwargs)\n",
    "            print(f\"📥 Read JSON: {filepath} ({len(df)} rows)\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to read {filepath}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test utility functions\n",
    "print(\"\\n🧪 Testing utility functions...\")\n",
    "\n",
    "# Test write_df\n",
    "test_csv_path = os.path.join(DATA_DIR_RAW, \"test_utilities.csv\")\n",
    "test_parquet_path = os.path.join(DATA_DIR_PROCESSED, \"test_utilities.parquet\")\n",
    "\n",
    "try:\n",
    "    # Test CSV\n",
    "    write_df(df_stock.head(10), test_csv_path)\n",
    "    \n",
    "    # Test Parquet\n",
    "    write_df(df_stock.head(10), test_parquet_path)\n",
    "    \n",
    "    # Test read_df\n",
    "    test_df_csv = read_df(test_csv_path)\n",
    "    test_df_parquet = read_df(test_parquet_path)\n",
    "    \n",
    "    print(\"✅ Utility functions work correctly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Utility function test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0a038f-7731-4db5-b01a-798c95b32832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>data_source</th>\n",
       "      <th>fetch_timestamp</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>volatility_20d</th>\n",
       "      <th>sma_20</th>\n",
       "      <th>sma_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>249.308990</td>\n",
       "      <td>253.381935</td>\n",
       "      <td>246.248259</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>76728215</td>\n",
       "      <td>simulated</td>\n",
       "      <td>2025-08-20 16:55:58.540384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>250.589112</td>\n",
       "      <td>254.414209</td>\n",
       "      <td>242.599626</td>\n",
       "      <td>249.433678</td>\n",
       "      <td>63440658</td>\n",
       "      <td>simulated</td>\n",
       "      <td>2025-08-20 16:55:58.540384</td>\n",
       "      <td>-0.002265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>251.556280</td>\n",
       "      <td>257.435396</td>\n",
       "      <td>249.039436</td>\n",
       "      <td>252.789502</td>\n",
       "      <td>51886947</td>\n",
       "      <td>simulated</td>\n",
       "      <td>2025-08-20 16:55:58.540384</td>\n",
       "      <td>0.013454</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>261.840595</td>\n",
       "      <td>267.990646</td>\n",
       "      <td>252.925690</td>\n",
       "      <td>260.616016</td>\n",
       "      <td>95111483</td>\n",
       "      <td>simulated</td>\n",
       "      <td>2025-08-20 16:55:58.540384</td>\n",
       "      <td>0.030961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>258.340223</td>\n",
       "      <td>264.002112</td>\n",
       "      <td>251.854899</td>\n",
       "      <td>259.525842</td>\n",
       "      <td>85174694</td>\n",
       "      <td>simulated</td>\n",
       "      <td>2025-08-20 16:55:58.540384</td>\n",
       "      <td>-0.004183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>259.451184</td>\n",
       "      <td>264.025664</td>\n",
       "      <td>252.359782</td>\n",
       "      <td>258.440313</td>\n",
       "      <td>68532681</td>\n",
       "      <td>simulated</td>\n",
       "      <td>2025-08-20 16:55:58.540384</td>\n",
       "      <td>-0.004183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-01-07</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>266.806066</td>\n",
       "      <td>272.772404</td>\n",
       "      <td>261.336950</td>\n",
       "      <td>266.732178</td>\n",
       "      <td>27667857</td>\n",
       "      <td>simulated</td>\n",
       "      <td>2025-08-20 16:55:58.540384</td>\n",
       "      <td>0.032084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-01-08</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>272.295270</td>\n",
       "      <td>273.740091</td>\n",
       "      <td>267.005733</td>\n",
       "      <td>270.959535</td>\n",
       "      <td>91537554</td>\n",
       "      <td>simulated</td>\n",
       "      <td>2025-08-20 16:55:58.540384</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>267.406270</td>\n",
       "      <td>274.800222</td>\n",
       "      <td>264.164309</td>\n",
       "      <td>268.550843</td>\n",
       "      <td>81279200</td>\n",
       "      <td>simulated</td>\n",
       "      <td>2025-08-20 16:55:58.540384</td>\n",
       "      <td>-0.008889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>271.745486</td>\n",
       "      <td>275.282299</td>\n",
       "      <td>265.873378</td>\n",
       "      <td>271.599218</td>\n",
       "      <td>95998646</td>\n",
       "      <td>simulated</td>\n",
       "      <td>2025-08-20 16:55:58.540384</td>\n",
       "      <td>0.011351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date symbol        open        high         low       close  \\\n",
       "0  2023-01-01   MSFT  249.308990  253.381935  246.248259  250.000000   \n",
       "1  2023-01-02   MSFT  250.589112  254.414209  242.599626  249.433678   \n",
       "2  2023-01-03   MSFT  251.556280  257.435396  249.039436  252.789502   \n",
       "3  2023-01-04   MSFT  261.840595  267.990646  252.925690  260.616016   \n",
       "4  2023-01-05   MSFT  258.340223  264.002112  251.854899  259.525842   \n",
       "5  2023-01-06   MSFT  259.451184  264.025664  252.359782  258.440313   \n",
       "6  2023-01-07   MSFT  266.806066  272.772404  261.336950  266.732178   \n",
       "7  2023-01-08   MSFT  272.295270  273.740091  267.005733  270.959535   \n",
       "8  2023-01-09   MSFT  267.406270  274.800222  264.164309  268.550843   \n",
       "9  2023-01-10   MSFT  271.745486  275.282299  265.873378  271.599218   \n",
       "\n",
       "     volume data_source             fetch_timestamp  daily_return  \\\n",
       "0  76728215   simulated  2025-08-20 16:55:58.540384           NaN   \n",
       "1  63440658   simulated  2025-08-20 16:55:58.540384     -0.002265   \n",
       "2  51886947   simulated  2025-08-20 16:55:58.540384      0.013454   \n",
       "3  95111483   simulated  2025-08-20 16:55:58.540384      0.030961   \n",
       "4  85174694   simulated  2025-08-20 16:55:58.540384     -0.004183   \n",
       "5  68532681   simulated  2025-08-20 16:55:58.540384     -0.004183   \n",
       "6  27667857   simulated  2025-08-20 16:55:58.540384      0.032084   \n",
       "7  91537554   simulated  2025-08-20 16:55:58.540384      0.015849   \n",
       "8  81279200   simulated  2025-08-20 16:55:58.540384     -0.008889   \n",
       "9  95998646   simulated  2025-08-20 16:55:58.540384      0.011351   \n",
       "\n",
       "   volatility_20d  sma_20  sma_50  \n",
       "0             NaN     NaN     NaN  \n",
       "1             NaN     NaN     NaN  \n",
       "2             NaN     NaN     NaN  \n",
       "3             NaN     NaN     NaN  \n",
       "4             NaN     NaN     NaN  \n",
       "5             NaN     NaN     NaN  \n",
       "6             NaN     NaN     NaN  \n",
       "7             NaN     NaN     NaN  \n",
       "8             NaN     NaN     NaN  \n",
       "9             NaN     NaN     NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617883d-99a9-4c59-b0bc-fd917cd5ce6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
