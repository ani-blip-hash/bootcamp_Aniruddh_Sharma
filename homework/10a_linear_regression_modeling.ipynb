{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 10a: Linear Regression Modeling\n",
    "\n",
    "**Assignment**: Build linear regression model with residual diagnostics and interpretation.\n",
    "\n",
    "## Objectives\n",
    "- Fit linear regression using engineered features\n",
    "- Perform comprehensive residual analysis\n",
    "- Interpret coefficients and model performance\n",
    "- Test model assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import utils\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üìà Homework 10a: Linear Regression Modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Engineered Features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with engineered features\n",
    "symbols = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
    "raw_data = utils.fetch_multiple_stocks(symbols, prefer_alphavantage=False, period='2y')\n",
    "\n",
    "if not raw_data.empty:\n",
    "    # Recreate engineered features (from homework 09)\n",
    "    processed_data = []\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        symbol_data = raw_data[raw_data['symbol'] == symbol].copy().sort_values('date')\n",
    "        \n",
    "        # Basic features\n",
    "        symbol_data['daily_return'] = symbol_data['close'].pct_change()\n",
    "        symbol_data['log_return'] = np.log(symbol_data['close'] / symbol_data['close'].shift(1))\n",
    "        \n",
    "        # Moving averages\n",
    "        symbol_data['sma_20'] = symbol_data['close'].rolling(20).mean()\n",
    "        symbol_data['volume_ma_20'] = symbol_data['volume'].rolling(20).mean()\n",
    "        \n",
    "        # Engineered features\n",
    "        rolling_vol = symbol_data['daily_return'].rolling(20).std()\n",
    "        symbol_data['vol_adj_return'] = symbol_data['daily_return'] / rolling_vol\n",
    "        \n",
    "        volume_ratio = symbol_data['volume'] / symbol_data['volume_ma_20']\n",
    "        price_momentum = symbol_data['close'].pct_change(10)\n",
    "        symbol_data['volume_momentum'] = volume_ratio * np.sign(price_momentum) * np.abs(price_momentum)\n",
    "        \n",
    "        # Target variable\n",
    "        symbol_data['target_return'] = symbol_data['daily_return'].shift(-1)\n",
    "        \n",
    "        processed_data.append(symbol_data)\n",
    "    \n",
    "    df = pd.concat(processed_data, ignore_index=True)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"‚úÖ Dataset loaded: {df.shape}\")\n",
    "    print(f\"Symbols: {df['symbol'].unique()}\")\n",
    "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\nelse:\n",
    "    print(\"‚ùå Failed to load data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Define feature columns\n",
    "    feature_cols = ['daily_return', 'vol_adj_return', 'volume_momentum']\n",
    "    target_col = 'target_return'\n",
    "    \n",
    "    # Create feature matrix and target vector\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # Remove any remaining NaN values\n",
    "    mask = ~(X.isna().any(axis=1) | y.isna())\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    print(f\"üìä Feature Matrix: {X.shape}\")\n",
    "    print(f\"üéØ Target Vector: {y.shape}\")\n",
    "    \n",
    "    # Feature statistics\n",
    "    print(\"\\nüìà Feature Statistics:\")\n",
    "    print(X.describe().round(4))\n",
    "    \n",
    "    print(f\"\\nüéØ Target Statistics:\")\n",
    "    print(f\"Mean: {y.mean():.6f}\")\n",
    "    print(f\"Std: {y.std():.6f}\")\n",
    "    print(f\"Range: [{y.min():.6f}, {y.max():.6f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Split data (80-20 split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f\"üîÑ Train set: {X_train.shape}\")\n",
    "    print(f\"üîÑ Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert back to DataFrame for easier handling\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
    "    \n",
    "    print(\"\\n‚öñÔ∏è Features scaled using StandardScaler\")\n",
    "    print(\"Scaled feature means (should be ~0):\")\n",
    "    print(X_train_scaled.mean().round(6))\n",
    "    print(\"Scaled feature stds (should be ~1):\")\n",
    "    print(X_train_scaled.std().round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Fit linear regression\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = lr_model.predict(X_train_scaled)\n",
    "    y_test_pred = lr_model.predict(X_test_scaled)\n",
    "    \n",
    "    print(\"üìà Linear Regression Model Fitted\")\n",
    "    print(f\"Intercept: {lr_model.intercept_:.6f}\")\n",
    "    print(\"\\nCoefficients:\")\n",
    "    for feature, coef in zip(feature_cols, lr_model.coef_):\n",
    "        print(f\"  {feature}: {coef:.6f}\")\n",
    "    \n",
    "    # Model performance metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    print(\"\\nüìä Model Performance:\")\n",
    "    print(f\"Train R¬≤: {train_r2:.6f}\")\n",
    "    print(f\"Test R¬≤: {test_r2:.6f}\")\n",
    "    print(f\"Train RMSE: {train_rmse:.6f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.6f}\")\n",
    "    print(f\"Train MAE: {train_mae:.6f}\")\n",
    "    print(f\"Test MAE: {test_mae:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Calculate residuals\n",
    "    train_residuals = y_train - y_train_pred\n",
    "    test_residuals = y_test - y_test_pred\n",
    "    \n",
    "    # Residual diagnostics plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0,0].scatter(y_train_pred, train_residuals, alpha=0.6, s=20)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[0,0].set_xlabel('Fitted Values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted Values')\n",
    "    \n",
    "    # 2. Q-Q Plot\n",
    "    stats.probplot(train_residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Q-Q Plot of Residuals')\n",
    "    \n",
    "    # 3. Histogram of residuals\n",
    "    axes[1,0].hist(train_residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Residuals')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    axes[1,0].set_title('Distribution of Residuals')\n",
    "    \n",
    "    # 4. Scale-Location plot\n",
    "    sqrt_abs_residuals = np.sqrt(np.abs(train_residuals))\n",
    "    axes[1,1].scatter(y_train_pred, sqrt_abs_residuals, alpha=0.6, s=20)\n",
    "    axes[1,1].set_xlabel('Fitted Values')\n",
    "    axes[1,1].set_ylabel('‚àö|Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Residual statistics\n",
    "    print(\"\\nüìä Residual Analysis:\")\n",
    "    print(f\"Mean residual: {train_residuals.mean():.8f} (should be ~0)\")\n",
    "    print(f\"Std residual: {train_residuals.std():.6f}\")\n",
    "    print(f\"Skewness: {stats.skew(train_residuals):.4f}\")\n",
    "    print(f\"Kurtosis: {stats.kurtosis(train_residuals):.4f}\")\n",
    "    \n",
    "    # Normality test\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(train_residuals[:5000])  # Limit sample size\n",
    "    print(f\"\\nShapiro-Wilk normality test:\")\n",
    "    print(f\"Statistic: {shapiro_stat:.6f}, p-value: {shapiro_p:.6f}\")\n",
    "    \n",
    "    # Durbin-Watson test for autocorrelation\n",
    "    def durbin_watson(residuals):\n",
    "        diff = np.diff(residuals)\n",
    "        return np.sum(diff**2) / np.sum(residuals**2)\n",
    "    \n",
    "    dw_stat = durbin_watson(train_residuals)\n",
    "    print(f\"\\nDurbin-Watson statistic: {dw_stat:.4f}\")\n",
    "    print(\"(Values around 2 indicate no autocorrelation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Feature importance visualization\n",
    "    coefficients = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Coefficient': lr_model.coef_,\n",
    "        'Abs_Coefficient': np.abs(lr_model.coef_)\n",
    "    }).sort_values('Abs_Coefficient', ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['red' if x < 0 else 'blue' for x in coefficients['Coefficient']]\n",
    "    plt.barh(coefficients['Feature'], coefficients['Coefficient'], color=colors, alpha=0.7)\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title('Linear Regression Coefficients')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüîç Model Interpretation:\")\n",
    "    print(\"\\nCoefficient Analysis:\")\n",
    "    for _, row in coefficients.iterrows():\n",
    "        direction = \"increases\" if row['Coefficient'] > 0 else \"decreases\"\n",
    "        print(f\"‚Ä¢ {row['Feature']}: {direction} target by {abs(row['Coefficient']):.6f} per unit increase\")\n",
    "    \n",
    "    # Prediction vs Actual scatter plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Training set\n",
    "    axes[0].scatter(y_train, y_train_pred, alpha=0.6, s=20)\n",
    "    axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "    axes[0].set_xlabel('Actual Returns')\n",
    "    axes[0].set_ylabel('Predicted Returns')\n",
    "    axes[0].set_title(f'Training Set (R¬≤ = {train_r2:.4f})')\n",
    "    \n",
    "    # Test set\n",
    "    axes[1].scatter(y_test, y_test_pred, alpha=0.6, s=20)\n",
    "    axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[1].set_xlabel('Actual Returns')\n",
    "    axes[1].set_ylabel('Predicted Returns')\n",
    "    axes[1].set_title(f'Test Set (R¬≤ = {test_r2:.4f})')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Model assumptions check\n",
    "    print(\"\\n‚úÖ Model Assumptions Check:\")\n",
    "    print(f\"1. Linearity: Check residuals vs fitted plot\")\n",
    "    print(f\"2. Independence: Durbin-Watson = {dw_stat:.4f}\")\n",
    "    print(f\"3. Homoscedasticity: Check scale-location plot\")\n",
    "    print(f\"4. Normality: Shapiro-Wilk p = {shapiro_p:.6f}\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\nüìà Model Performance Summary:\")\n",
    "    print(f\"‚Ä¢ Explains {test_r2*100:.2f}% of variance in test set\")\n",
    "    print(f\"‚Ä¢ Average prediction error: {test_mae:.6f}\")\n",
    "    print(f\"‚Ä¢ Root mean squared error: {test_rmse:.6f}\")\n",
    "    \n",
    "    overfitting = train_r2 - test_r2\n",
    "    if overfitting > 0.05:\n",
    "        print(f\"‚ö†Ô∏è Potential overfitting detected (train R¬≤ - test R¬≤ = {overfitting:.4f})\")\n",
    "    else:\n",
    "        print(f\"‚úÖ No significant overfitting (train R¬≤ - test R¬≤ = {overfitting:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Validation and Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Cross-validation by symbol\n",
    "    print(\"\\nüîÑ Cross-Validation by Symbol:\")\n",
    "    \n",
    "    symbol_performance = []\n",
    "    \n",
    "    for symbol in df['symbol'].unique():\n",
    "        # Get symbol data\n",
    "        symbol_mask = df[mask]['symbol'] == symbol\n",
    "        symbol_indices = df[mask][symbol_mask].index\n",
    "        \n",
    "        if len(symbol_indices) > 50:  # Minimum data points\n",
    "            X_symbol = X.loc[symbol_indices]\n",
    "            y_symbol = y.loc[symbol_indices]\n",
    "            \n",
    "            # Scale features\n",
    "            X_symbol_scaled = scaler.transform(X_symbol)\n",
    "            \n",
    "            # Predict\n",
    "            y_symbol_pred = lr_model.predict(X_symbol_scaled)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            symbol_r2 = r2_score(y_symbol, y_symbol_pred)\n",
    "            symbol_rmse = np.sqrt(mean_squared_error(y_symbol, y_symbol_pred))\n",
    "            \n",
    "            symbol_performance.append({\n",
    "                'Symbol': symbol,\n",
    "                'R¬≤': symbol_r2,\n",
    "                'RMSE': symbol_rmse,\n",
    "                'N_samples': len(symbol_indices)\n",
    "            })\n",
    "    \n",
    "    symbol_df = pd.DataFrame(symbol_performance)\n",
    "    print(symbol_df.round(4))\n",
    "    \n",
    "    # Feature stability across symbols\n",
    "    print(\"\\nüéØ Model Consistency:\")\n",
    "    print(f\"R¬≤ range: {symbol_df['R¬≤'].min():.4f} to {symbol_df['R¬≤'].max():.4f}\")\n",
    "    print(f\"R¬≤ std: {symbol_df['R¬≤'].std():.4f}\")\n",
    "    print(f\"Average R¬≤: {symbol_df['R¬≤'].mean():.4f}\")\n",
    "    \n",
    "    # Save model results\n",
    "    model_results = {\n",
    "        'coefficients': dict(zip(feature_cols, lr_model.coef_)),\n",
    "        'intercept': lr_model.intercept_,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'features_used': feature_cols\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüíæ Model Results Summary:\")\n",
    "    for key, value in model_results.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"{key}: {value:.6f}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions\n",
    "\n",
    "### Model Performance\n",
    "- **R¬≤ Score**: Measures proportion of variance explained\n",
    "- **RMSE**: Root mean squared error in same units as target\n",
    "- **MAE**: Mean absolute error, robust to outliers\n",
    "\n",
    "### Key Findings\n",
    "1. **Feature Importance**: Coefficients show relative impact of each feature\n",
    "2. **Model Assumptions**: Residual analysis reveals assumption violations\n",
    "3. **Generalization**: Cross-validation shows model consistency across symbols\n",
    "\n",
    "### Limitations\n",
    "- Linear models assume linear relationships\n",
    "- Financial returns often exhibit non-linear patterns\n",
    "- Model may not capture regime changes or volatility clustering\n",
    "\n",
    "### Next Steps\n",
    "1. **Feature Engineering**: Add interaction terms or polynomial features\n",
    "2. **Regularization**: Try Ridge/Lasso regression to prevent overfitting\n",
    "3. **Non-linear Models**: Consider tree-based or neural network models\n",
    "4. **Time Series**: Account for temporal dependencies in data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
