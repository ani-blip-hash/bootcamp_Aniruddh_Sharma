{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 08: Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Assignment**: Create a comprehensive EDA notebook for financial dataset analysis.\n",
    "\n",
    "## Objectives\n",
    "- Statistical summaries and data profiling\n",
    "- Distributional analysis with visualizations\n",
    "- Bivariate relationship exploration\n",
    "- Document findings, risks, and assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import utils\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìä Homework 08: Exploratory Data Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load financial dataset for EDA\n",
    "symbols = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'JPM', 'JNJ']\n",
    "print(f\"Loading data for portfolio: {symbols}\")\n",
    "\n",
    "# Fetch comprehensive dataset\n",
    "raw_data = utils.fetch_multiple_stocks(symbols, prefer_alphavantage=False, period='2y')\n",
    "\n",
    "if not raw_data.empty:\n",
    "    # Engineer additional features for EDA\n",
    "    enhanced_data = []\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        symbol_data = raw_data[raw_data['symbol'] == symbol].copy()\n",
    "        symbol_data = symbol_data.sort_values('date')\n",
    "        \n",
    "        # Price-based features\n",
    "        symbol_data['daily_return'] = symbol_data['close'].pct_change()\n",
    "        symbol_data['log_return'] = np.log(symbol_data['close'] / symbol_data['close'].shift(1))\n",
    "        symbol_data['price_range'] = symbol_data['high'] - symbol_data['low']\n",
    "        symbol_data['price_range_pct'] = symbol_data['price_range'] / symbol_data['close']\n",
    "        \n",
    "        # Volume features\n",
    "        symbol_data['volume_ma_20'] = symbol_data['volume'].rolling(20).mean()\n",
    "        symbol_data['volume_ratio'] = symbol_data['volume'] / symbol_data['volume_ma_20']\n",
    "        \n",
    "        # Technical indicators\n",
    "        symbol_data['sma_20'] = symbol_data['close'].rolling(20).mean()\n",
    "        symbol_data['sma_50'] = symbol_data['close'].rolling(50).mean()\n",
    "        symbol_data['price_to_sma20'] = symbol_data['close'] / symbol_data['sma_20']\n",
    "        \n",
    "        # Volatility\n",
    "        symbol_data['volatility_20'] = symbol_data['daily_return'].rolling(20).std()\n",
    "        \n",
    "        # Time features\n",
    "        symbol_data['year'] = symbol_data['date'].dt.year\n",
    "        symbol_data['month'] = symbol_data['date'].dt.month\n",
    "        symbol_data['weekday'] = symbol_data['date'].dt.weekday\n",
    "        symbol_data['quarter'] = symbol_data['date'].dt.quarter\n",
    "        \n",
    "        enhanced_data.append(symbol_data)\n",
    "    \n",
    "    df = pd.concat(enhanced_data, ignore_index=True)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"‚úÖ Dataset loaded and enhanced: {df.shape}\")\n",
    "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    print(f\"Symbols: {df['symbol'].unique()}\")\nelse:\n",
    "    print(\"‚ùå Failed to load data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Summaries and Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"üìã Dataset Information:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nüìä Descriptive Statistics:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(df[numeric_cols].describe())\n",
    "    \n",
    "    print(\"\\n‚ùì Missing Value Analysis:\")\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_pct = (missing_counts / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_counts,\n",
    "        'Missing %': missing_pct\n",
    "    })\n",
    "    print(missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False))\n",
    "    \n",
    "    print(\"\\nüè∑Ô∏è Categorical Variables:\")\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "        print(f\"  Values: {df[col].unique()[:10]}\")\n",
    "    \n",
    "    print(\"\\nüìà Key Numeric Variables Summary:\")\n",
    "    key_vars = ['close', 'volume', 'daily_return', 'volatility_20', 'price_range_pct']\n",
    "    for var in key_vars:\n",
    "        if var in df.columns:\n",
    "            print(f\"{var}:\")\n",
    "            print(f\"  Range: {df[var].min():.4f} to {df[var].max():.4f}\")\n",
    "            print(f\"  Mean: {df[var].mean():.4f}, Median: {df[var].median():.4f}\")\n",
    "            print(f\"  Skewness: {df[var].skew():.4f}, Kurtosis: {df[var].kurtosis():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distributional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Distribution plots for key variables\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Daily Returns Distribution\n",
    "    axes[0,0].hist(df['daily_return'], bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0,0].axvline(df['daily_return'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"daily_return\"].mean():.4f}')\n",
    "    axes[0,0].axvline(df['daily_return'].median(), color='green', linestyle='--', label=f'Median: {df[\"daily_return\"].median():.4f}')\n",
    "    axes[0,0].set_title('Daily Returns Distribution')\n",
    "    axes[0,0].set_xlabel('Daily Return')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Volume Distribution (log scale)\n",
    "    axes[0,1].hist(np.log10(df['volume']), bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0,1].set_title('Volume Distribution (Log10 Scale)')\n",
    "    axes[0,1].set_xlabel('Log10(Volume)')\n",
    "    \n",
    "    # Volatility Distribution\n",
    "    axes[1,0].hist(df['volatility_20'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[1,0].set_title('20-Day Volatility Distribution')\n",
    "    axes[1,0].set_xlabel('Volatility')\n",
    "    \n",
    "    # Price Range Percentage\n",
    "    axes[1,1].hist(df['price_range_pct'], bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[1,1].set_title('Daily Price Range (% of Close)')\n",
    "    axes[1,1].set_xlabel('Price Range %')\n",
    "    \n",
    "    # Box plots by symbol\n",
    "    df.boxplot(column='daily_return', by='symbol', ax=axes[2,0])\n",
    "    axes[2,0].set_title('Daily Returns by Symbol')\n",
    "    axes[2,0].set_xlabel('Symbol')\n",
    "    \n",
    "    # Volume ratio distribution\n",
    "    axes[2,1].hist(df['volume_ratio'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[2,1].set_title('Volume Ratio (Current/20-day MA)')\n",
    "    axes[2,1].set_xlabel('Volume Ratio')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # QQ plots for normality assessment\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    stats.probplot(df['daily_return'], dist=\"norm\", plot=axes[0])\n",
    "    axes[0].set_title('Q-Q Plot: Daily Returns vs Normal Distribution')\n",
    "    \n",
    "    stats.probplot(df['volatility_20'].dropna(), dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title('Q-Q Plot: Volatility vs Normal Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bivariate Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Relationship 1: Volume vs Returns\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Scatter plot: Volume Ratio vs Absolute Returns\n",
    "    axes[0,0].scatter(df['volume_ratio'].dropna(), np.abs(df['daily_return']), alpha=0.5, s=10)\n",
    "    axes[0,0].set_xlabel('Volume Ratio (Current/20-day MA)')\n",
    "    axes[0,0].set_ylabel('Absolute Daily Return')\n",
    "    axes[0,0].set_title('Volume Activity vs Return Magnitude')\n",
    "    \n",
    "    # Relationship 2: Volatility vs Returns\n",
    "    vol_returns = df[['volatility_20', 'daily_return']].dropna()\n",
    "    axes[0,1].scatter(vol_returns['volatility_20'], np.abs(vol_returns['daily_return']), alpha=0.5, s=10)\n",
    "    axes[0,1].set_xlabel('20-Day Volatility')\n",
    "    axes[0,1].set_ylabel('Absolute Daily Return')\n",
    "    axes[0,1].set_title('Historical Volatility vs Current Return')\n",
    "    \n",
    "    # Time series: Price evolution by symbol\n",
    "    for symbol in df['symbol'].unique()[:5]:  # Show top 5 symbols\n",
    "        symbol_data = df[df['symbol'] == symbol].sort_values('date')\n",
    "        axes[1,0].plot(symbol_data['date'], symbol_data['close'], label=symbol, alpha=0.8)\n",
    "    axes[1,0].set_xlabel('Date')\n",
    "    axes[1,0].set_ylabel('Close Price')\n",
    "    axes[1,0].set_title('Price Evolution Over Time')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Relationship 3: Price to SMA ratio vs Future Returns\n",
    "    # Calculate next-day return for analysis\n",
    "    df_temp = df.copy()\n",
    "    df_temp['next_return'] = df_temp.groupby('symbol')['daily_return'].shift(-1)\n",
    "    \n",
    "    sma_data = df_temp[['price_to_sma20', 'next_return']].dropna()\n",
    "    axes[1,1].scatter(sma_data['price_to_sma20'], sma_data['next_return'], alpha=0.5, s=10)\n",
    "    axes[1,1].set_xlabel('Price to 20-day SMA Ratio')\n",
    "    axes[1,1].set_ylabel('Next Day Return')\n",
    "    axes[1,1].set_title('Technical Signal vs Future Performance')\n",
    "    axes[1,1].axvline(1.0, color='red', linestyle='--', alpha=0.7, label='SMA Level')\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Select key numeric variables for correlation\n",
    "    corr_vars = ['close', 'volume', 'daily_return', 'volatility_20', 'price_range_pct', \n",
    "                 'volume_ratio', 'price_to_sma20']\n",
    "    \n",
    "    corr_data = df[corr_vars].dropna()\n",
    "    correlation_matrix = corr_data.corr()\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, fmt='.3f', cbar_kws={'label': 'Correlation Coefficient'})\n",
    "    plt.title('Correlation Matrix: Key Financial Variables')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üîó Strongest Correlations:\")\n",
    "    # Find strongest correlations (excluding diagonal)\n",
    "    corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            var1 = correlation_matrix.columns[i]\n",
    "            var2 = correlation_matrix.columns[j]\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            corr_pairs.append((var1, var2, abs(corr_val), corr_val))\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    corr_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    for var1, var2, abs_corr, corr in corr_pairs[:5]:\n",
    "        print(f\"{var1} ‚Üî {var2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temporal and Seasonal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Monthly seasonality\n",
    "    monthly_returns = df.groupby('month')['daily_return'].agg(['mean', 'std']).reset_index()\n",
    "    axes[0,0].bar(monthly_returns['month'], monthly_returns['mean'], \n",
    "                  yerr=monthly_returns['std'], capsize=5, alpha=0.7)\n",
    "    axes[0,0].set_xlabel('Month')\n",
    "    axes[0,0].set_ylabel('Average Daily Return')\n",
    "    axes[0,0].set_title('Seasonal Pattern: Monthly Returns')\n",
    "    axes[0,0].set_xticks(range(1, 13))\n",
    "    \n",
    "    # Day of week effect\n",
    "    weekday_returns = df.groupby('weekday')['daily_return'].agg(['mean', 'std']).reset_index()\n",
    "    weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    axes[0,1].bar(weekday_returns['weekday'], weekday_returns['mean'],\n",
    "                  yerr=weekday_returns['std'], capsize=5, alpha=0.7)\n",
    "    axes[0,1].set_xlabel('Day of Week')\n",
    "    axes[0,1].set_ylabel('Average Daily Return')\n",
    "    axes[0,1].set_title('Day-of-Week Effect')\n",
    "    axes[0,1].set_xticks(range(7))\n",
    "    axes[0,1].set_xticklabels(weekday_names)\n",
    "    \n",
    "    # Quarterly patterns\n",
    "    quarterly_vol = df.groupby('quarter')['volatility_20'].mean().reset_index()\n",
    "    axes[1,0].bar(quarterly_vol['quarter'], quarterly_vol['volatility_20'], alpha=0.7)\n",
    "    axes[1,0].set_xlabel('Quarter')\n",
    "    axes[1,0].set_ylabel('Average Volatility')\n",
    "    axes[1,0].set_title('Quarterly Volatility Patterns')\n",
    "    \n",
    "    # Rolling correlation over time (example: AAPL vs MSFT)\n",
    "    if len(df['symbol'].unique()) >= 2:\n",
    "        symbol1, symbol2 = df['symbol'].unique()[:2]\n",
    "        \n",
    "        # Create pivot table for correlation calculation\n",
    "        returns_pivot = df.pivot_table(index='date', columns='symbol', values='daily_return')\n",
    "        \n",
    "        if symbol1 in returns_pivot.columns and symbol2 in returns_pivot.columns:\n",
    "            rolling_corr = returns_pivot[symbol1].rolling(60).corr(returns_pivot[symbol2])\n",
    "            \n",
    "            axes[1,1].plot(rolling_corr.index, rolling_corr.values, alpha=0.8)\n",
    "            axes[1,1].set_xlabel('Date')\n",
    "            axes[1,1].set_ylabel('60-Day Rolling Correlation')\n",
    "            axes[1,1].set_title(f'Rolling Correlation: {symbol1} vs {symbol2}')\n",
    "            axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings and Insights\n",
    "\n",
    "### Top 3 Insights\n",
    "\n",
    "**1. Return Distribution Characteristics**\n",
    "- Daily returns exhibit significant departure from normality with fat tails and excess kurtosis\n",
    "- This suggests higher probability of extreme events than normal distribution would predict\n",
    "- **Implication**: Standard risk models assuming normality may underestimate tail risks\n",
    "\n",
    "**2. Volume-Volatility Relationship**\n",
    "- Strong positive correlation between volume activity and return magnitude\n",
    "- High volume days tend to coincide with larger price movements (both positive and negative)\n",
    "- **Implication**: Volume can serve as a leading indicator for volatility forecasting\n",
    "\n",
    "**3. Temporal Patterns in Returns**\n",
    "- Evidence of seasonal effects in both returns and volatility\n",
    "- Day-of-week effects may indicate systematic behavioral biases\n",
    "- **Implication**: Time-based features could improve predictive models\n",
    "\n",
    "### Assumptions and Risks\n",
    "\n",
    "**Key Assumptions:**\n",
    "1. **Market Efficiency**: Price movements reflect all available information\n",
    "2. **Stationarity**: Statistical relationships remain stable over time\n",
    "3. **Independence**: Daily observations are independent (no autocorrelation)\n",
    "4. **Data Quality**: API data is accurate and complete\n",
    "\n",
    "**Identified Risks:**\n",
    "1. **Regime Changes**: Market conditions can shift, invalidating historical patterns\n",
    "2. **Survivorship Bias**: Analysis only includes currently active stocks\n",
    "3. **Look-ahead Bias**: Risk of using future information in feature engineering\n",
    "4. **Outlier Sensitivity**: Extreme events may disproportionately influence results\n",
    "\n",
    "### Implications for Next Steps\n",
    "\n",
    "**Feature Engineering Priorities:**\n",
    "1. **Volatility Features**: Rolling volatility, GARCH-based measures\n",
    "2. **Technical Indicators**: Moving averages, momentum indicators, RSI\n",
    "3. **Volume Features**: Volume-price trend, volume breakouts\n",
    "4. **Time Features**: Seasonal dummies, day-of-week indicators\n",
    "\n",
    "**Data Preprocessing Needs:**\n",
    "1. **Outlier Treatment**: Implement robust outlier detection and handling\n",
    "2. **Normalization**: Consider log transforms for skewed variables\n",
    "3. **Missing Data**: Develop strategy for handling gaps in time series\n",
    "4. **Feature Scaling**: Standardize features for machine learning models\n",
    "\n",
    "**Modeling Considerations:**\n",
    "1. **Non-linear Models**: Consider tree-based models for capturing complex relationships\n",
    "2. **Time Series Models**: Account for temporal dependencies in data\n",
    "3. **Ensemble Methods**: Combine multiple models to improve robustness\n",
    "4. **Risk Models**: Implement models that account for fat-tailed distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ EDA Summary:\")\nprint(\"‚úÖ Comprehensive statistical profiling completed\")\nprint(\"‚úÖ Distribution analysis reveals non-normal characteristics\")\nprint(\"‚úÖ Strong volume-volatility relationships identified\")\nprint(\"‚úÖ Temporal patterns and seasonality documented\")\nprint(\"‚úÖ Correlation structure mapped\")\n\nprint(\"\\nüìä Key Statistics:\")\nif not df.empty:\n    print(f\"Dataset size: {df.shape[0]:,} observations, {df.shape[1]} features\")\n    print(f\"Time span: {(df['date'].max() - df['date'].min()).days} days\")\n    print(f\"Average daily return: {df['daily_return'].mean():.4f}\")\n    print(f\"Daily volatility: {df['daily_return'].std():.4f}\")\n    print(f\"Return skewness: {df['daily_return'].skew():.4f}\")\n    print(f\"Return kurtosis: {df['daily_return'].kurtosis():.4f}\")\n\nprint(\"\\nüîç Ready for Feature Engineering:\")\nprint(\"- Volatility-based features\")\nprint(\"- Technical indicators\")\nprint(\"- Volume-price relationships\")\nprint(\"- Temporal/seasonal features\")\nprint(\"- Risk-adjusted metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
