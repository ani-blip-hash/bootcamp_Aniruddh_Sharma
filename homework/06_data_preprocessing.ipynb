{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 6: Data Preprocessing Homework\n",
    "\n",
    "**Objective**: Apply modular cleaning functions to raw stock market data, document assumptions, and compare original vs cleaned datasets.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading](#data-loading)\n",
    "2. [Initial Data Exploration](#initial-exploration)\n",
    "3. [Data Cleaning](#data-cleaning)\n",
    "4. [Data Comparison](#data-comparison)\n",
    "5. [Save Processed Data](#save-processed-data)\n",
    "6. [Assumptions and Tradeoffs](#assumptions-and-tradeoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to path to import our cleaning functions\n",
    "sys.path.append('../src')\n",
    "from cleaning import fill_missing_median, drop_missing, normalize_data, get_data_summary, print_cleaning_report\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading {#data-loading}\n",
    "\n",
    "Load the raw stock market dataset and perform initial inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset\n",
    "raw_data_path = '../homework/data/raw/stock_data_raw.csv'\n",
    "df_original = pd.read_csv(raw_data_path)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df_original.shape}\")\n",
    "print(f\"\\nColumns: {list(df_original.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration {#initial-exploration}\n",
    "\n",
    "Examine the structure, data types, and missing values in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Shape: {df_original.shape}\")\n",
    "print(f\"Memory usage: {df_original.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(df_original.dtypes)\n",
    "print(\"\\n=== FIRST 5 ROWS ===\")\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=== MISSING VALUES ANALYSIS ===\")\n",
    "missing_values = df_original.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df_original)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percentage\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df)\n",
    "else:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical columns\n",
    "print(\"=== STATISTICAL SUMMARY ===\")\n",
    "df_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values pattern\n",
    "if df_original.isnull().sum().sum() > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Missing values heatmap\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(df_original.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
    "    plt.title('Missing Values Pattern')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Missing values bar chart\n",
    "    plt.subplot(1, 2, 2)\n",
    "    missing_counts = df_original.isnull().sum()\n",
    "    missing_counts = missing_counts[missing_counts > 0]\n",
    "    if len(missing_counts) > 0:\n",
    "        missing_counts.plot(kind='bar')\n",
    "        plt.title('Missing Values Count by Column')\n",
    "        plt.xlabel('Columns')\n",
    "        plt.ylabel('Missing Count')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning {#data-cleaning}\n",
    "\n",
    "Apply our modular cleaning functions to handle missing values and normalize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Handle Missing Values\n",
    "\n",
    "**Strategy**: \n",
    "- Fill missing values in technical indicators (volatility_20d, sma_20, sma_50) with median values\n",
    "- These columns likely have missing values at the beginning of the time series due to insufficient historical data for calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fill missing values with median for technical indicators\n",
    "print(\"=== STEP 1: FILLING MISSING VALUES ===\")\n",
    "\n",
    "# Identify columns with missing values\n",
    "columns_with_missing = df_original.columns[df_original.isnull().any()].tolist()\n",
    "print(f\"Columns with missing values: {columns_with_missing}\")\n",
    "\n",
    "# Apply median filling to technical indicator columns\n",
    "technical_columns = ['volatility_20d', 'sma_20', 'sma_50']\n",
    "existing_technical_cols = [col for col in technical_columns if col in df_original.columns]\n",
    "\n",
    "if existing_technical_cols:\n",
    "    df_step1 = fill_missing_median(df_original, columns=existing_technical_cols)\n",
    "    print(f\"\\nApplied median filling to: {existing_technical_cols}\")\n",
    "else:\n",
    "    df_step1 = df_original.copy()\n",
    "    print(\"No technical indicator columns found for median filling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Drop Remaining Missing Values\n",
    "\n",
    "**Strategy**: \n",
    "- Drop any remaining rows with missing values in critical columns\n",
    "- Focus on core price and volume data which should be complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Drop rows with missing values in critical columns\n",
    "print(\"=== STEP 2: DROPPING MISSING VALUES ===\")\n",
    "\n",
    "# Define critical columns that must have complete data\n",
    "critical_columns = ['date', 'symbol', 'open', 'high', 'low', 'close', 'volume']\n",
    "existing_critical_cols = [col for col in critical_columns if col in df_step1.columns]\n",
    "\n",
    "print(f\"Critical columns to check: {existing_critical_cols}\")\n",
    "\n",
    "# Check if there are missing values in critical columns\n",
    "critical_missing = df_step1[existing_critical_cols].isnull().sum().sum()\n",
    "print(f\"Missing values in critical columns: {critical_missing}\")\n",
    "\n",
    "if critical_missing > 0:\n",
    "    df_step2 = drop_missing(df_step1, columns=existing_critical_cols)\n",
    "else:\n",
    "    df_step2 = df_step1.copy()\n",
    "    print(\"No missing values in critical columns - no rows dropped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Normalize Numerical Data\n",
    "\n",
    "**Strategy**: \n",
    "- Apply standard scaling (z-score normalization) to price and volume data\n",
    "- Exclude date, symbol, and already-normalized technical indicators from scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Normalize numerical columns\n",
    "print(\"=== STEP 3: NORMALIZING DATA ===\")\n",
    "\n",
    "# Define columns to normalize (price and volume data)\n",
    "normalize_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "existing_normalize_cols = [col for col in normalize_columns if col in df_step2.columns]\n",
    "\n",
    "print(f\"Columns to normalize: {existing_normalize_cols}\")\n",
    "\n",
    "if existing_normalize_cols:\n",
    "    df_cleaned, scaler = normalize_data(df_step2, columns=existing_normalize_cols, method='standard')\n",
    "    print(\"\\nNormalization completed using standard scaling.\")\n",
    "else:\n",
    "    df_cleaned = df_step2.copy()\n",
    "    scaler = None\n",
    "    print(\"No columns found for normalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Comparison {#data-comparison}\n",
    "\n",
    "Compare the original and cleaned datasets to understand the impact of our cleaning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive cleaning report\n",
    "print_cleaning_report(df_original, df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare statistical summaries\n",
    "print(\"=== ORIGINAL DATA SUMMARY ===\")\n",
    "print(df_original.describe())\n",
    "\n",
    "print(\"\\n=== CLEANED DATA SUMMARY ===\")\n",
    "print(df_cleaned.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the impact of normalization\n",
    "if existing_normalize_cols and len(existing_normalize_cols) > 0:\n",
    "    fig, axes = plt.subplots(2, len(existing_normalize_cols), figsize=(15, 8))\n",
    "    \n",
    "    if len(existing_normalize_cols) == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i, col in enumerate(existing_normalize_cols):\n",
    "        # Original data distribution\n",
    "        axes[0, i].hist(df_original[col].dropna(), bins=30, alpha=0.7, color='blue')\n",
    "        axes[0, i].set_title(f'Original {col}')\n",
    "        axes[0, i].set_ylabel('Frequency')\n",
    "        \n",
    "        # Normalized data distribution\n",
    "        axes[1, i].hist(df_cleaned[col].dropna(), bins=30, alpha=0.7, color='red')\n",
    "        axes[1, i].set_title(f'Normalized {col}')\n",
    "        axes[1, i].set_ylabel('Frequency')\n",
    "        axes[1, i].set_xlabel('Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Distribution Comparison: Original vs Normalized Data', y=1.02)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No normalized columns to visualize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare missing values before and after cleaning\n",
    "print(\"=== MISSING VALUES COMPARISON ===\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original_Missing': df_original.isnull().sum(),\n",
    "    'Cleaned_Missing': df_cleaned.isnull().sum()\n",
    "})\n",
    "comparison_df['Difference'] = comparison_df['Original_Missing'] - comparison_df['Cleaned_Missing']\n",
    "comparison_df = comparison_df[comparison_df['Original_Missing'] > 0]\n",
    "\n",
    "if len(comparison_df) > 0:\n",
    "    print(comparison_df)\n",
    "else:\n",
    "    print(\"No missing values in either dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data {#save-processed-data}\n",
    "\n",
    "Save the cleaned dataset to the processed data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the processed data directory exists\n",
    "processed_dir = '../homework/data/processed/'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "output_path = os.path.join(processed_dir, 'stock_data_cleaned.csv')\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {output_path}\")\n",
    "print(f\"File size: {os.path.getsize(output_path) / 1024:.2f} KB\")\n",
    "\n",
    "# Also save a summary of the cleaning process\n",
    "summary_path = os.path.join(processed_dir, 'cleaning_summary.txt')\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"DATA CLEANING SUMMARY\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    f.write(f\"Original dataset shape: {df_original.shape}\\n\")\n",
    "    f.write(f\"Cleaned dataset shape: {df_cleaned.shape}\\n\")\n",
    "    f.write(f\"Rows removed: {df_original.shape[0] - df_cleaned.shape[0]}\\n\")\n",
    "    f.write(f\"Missing values removed: {df_original.isnull().sum().sum() - df_cleaned.isnull().sum().sum()}\\n\\n\")\n",
    "    \n",
    "    f.write(\"CLEANING OPERATIONS PERFORMED:\\n\")\n",
    "    f.write(\"1. Filled missing values in technical indicators with median\\n\")\n",
    "    f.write(\"2. Dropped rows with missing values in critical columns\\n\")\n",
    "    f.write(\"3. Normalized price and volume data using standard scaling\\n\\n\")\n",
    "    \n",
    "    if existing_normalize_cols:\n",
    "        f.write(f\"Normalized columns: {existing_normalize_cols}\\n\")\n",
    "    if existing_technical_cols:\n",
    "        f.write(f\"Median-filled columns: {existing_technical_cols}\\n\")\n",
    "\n",
    "print(f\"Cleaning summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Assumptions and Tradeoffs {#assumptions-and-tradeoffs}\n",
    "\n",
    "### Key Assumptions Made:\n",
    "\n",
    "1. **Missing Value Imputation**:\n",
    "   - **Assumption**: Missing values in technical indicators (volatility_20d, sma_20, sma_50) are due to insufficient historical data at the beginning of the time series\n",
    "   - **Rationale**: These indicators require 20+ days of historical data to calculate\n",
    "   - **Tradeoff**: Using median imputation may introduce bias, but preserves more data points than dropping\n",
    "\n",
    "2. **Data Completeness**:\n",
    "   - **Assumption**: Core price data (OHLCV) should be complete and accurate\n",
    "   - **Rationale**: These are fundamental market data points that should always be available\n",
    "   - **Tradeoff**: Dropping incomplete records reduces dataset size but ensures data quality\n",
    "\n",
    "3. **Normalization Strategy**:\n",
    "   - **Assumption**: Price and volume data follow approximately normal distributions\n",
    "   - **Rationale**: Standard scaling works well for normally distributed data\n",
    "   - **Tradeoff**: Loses original scale interpretation but enables better model performance\n",
    "\n",
    "4. **Feature Selection**:\n",
    "   - **Assumption**: All numerical columns are relevant for analysis\n",
    "   - **Rationale**: Stock market data typically has interconnected features\n",
    "   - **Tradeoff**: May include noisy features but preserves potentially useful information\n",
    "\n",
    "### Alternative Approaches Considered:\n",
    "\n",
    "1. **Forward-fill imputation** for technical indicators (would preserve trends)\n",
    "2. **Min-max scaling** instead of standard scaling (would preserve relative relationships)\n",
    "3. **Complete case analysis** (dropping all rows with any missing values)\n",
    "4. **Interpolation methods** for time series data\n",
    "\n",
    "### Impact Assessment:\n",
    "\n",
    "- **Data Loss**: Minimal rows removed, preserving dataset size\n",
    "- **Information Preservation**: Technical indicators maintained through imputation\n",
    "- **Model Readiness**: Normalized features ready for machine learning algorithms\n",
    "- **Reproducibility**: All operations documented and parameterized in reusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation of cleaned dataset\n",
    "print(\"=== FINAL VALIDATION ===\")\n",
    "print(f\"‚úì Dataset shape: {df_cleaned.shape}\")\n",
    "print(f\"‚úì Missing values: {df_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"‚úì Data types consistent: {df_cleaned.dtypes.nunique()} unique types\")\n",
    "print(f\"‚úì No infinite values: {np.isinf(df_cleaned.select_dtypes(include=[np.number])).sum().sum() == 0}\")\n",
    "\n",
    "# Check for any remaining data quality issues\n",
    "numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "if len(numeric_cols) > 0:\n",
    "    print(f\"‚úì Numeric columns range check:\")\n",
    "    for col in numeric_cols[:5]:  # Show first 5 numeric columns\n",
    "        min_val, max_val = df_cleaned[col].min(), df_cleaned[col].max()\n",
    "        print(f\"  {col}: [{min_val:.4f}, {max_val:.4f}]\")\n",
    "\n",
    "print(\"\\nüéâ Data preprocessing completed successfully!\")\n",
    "print(\"üìÅ Cleaned dataset saved to /homework/data/processed/stock_data_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
