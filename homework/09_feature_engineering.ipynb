{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 09: Feature Engineering\n",
    "\n",
    "**Assignment**: Create 2-3 engineered features from financial dataset with clear rationale and documentation.\n",
    "\n",
    "## Objectives\n",
    "- Implement engineered features based on EDA insights\n",
    "- Document reasoning for each feature\n",
    "- Test correlation with target variables\n",
    "- Create reusable feature engineering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üîß Homework 09: Feature Engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load financial dataset\n",
    "symbols = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
    "print(f\"Loading data for: {symbols}\")\n",
    "\n",
    "raw_data = utils.fetch_multiple_stocks(symbols, prefer_alphavantage=False, period='2y')\n",
    "\n",
    "if not raw_data.empty:\n",
    "    # Basic preprocessing\n",
    "    processed_data = []\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        symbol_data = raw_data[raw_data['symbol'] == symbol].copy()\n",
    "        symbol_data = symbol_data.sort_values('date')\n",
    "        \n",
    "        # Basic features\n",
    "        symbol_data['daily_return'] = symbol_data['close'].pct_change()\n",
    "        symbol_data['log_return'] = np.log(symbol_data['close'] / symbol_data['close'].shift(1))\n",
    "        symbol_data['price_range'] = symbol_data['high'] - symbol_data['low']\n",
    "        \n",
    "        # Moving averages for feature engineering\n",
    "        symbol_data['sma_5'] = symbol_data['close'].rolling(5).mean()\n",
    "        symbol_data['sma_20'] = symbol_data['close'].rolling(20).mean()\n",
    "        symbol_data['sma_50'] = symbol_data['close'].rolling(50).mean()\n",
    "        \n",
    "        # Volume moving average\n",
    "        symbol_data['volume_ma_20'] = symbol_data['volume'].rolling(20).mean()\n",
    "        \n",
    "        processed_data.append(symbol_data)\n",
    "    \n",
    "    df = pd.concat(processed_data, ignore_index=True)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"‚úÖ Dataset prepared: {df.shape}\")\n",
    "    print(f\"Base features: {list(df.columns)}\")\nelse:\n",
    "    print(\"‚ùå Failed to load data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature 1: Volatility-Adjusted Return Ratio\n",
    "\n",
    "**Rationale**: From EDA, we observed that returns and volatility are closely related. This feature captures risk-adjusted performance by normalizing returns by their recent volatility, similar to a Sharpe ratio but using rolling volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_volatility_adjusted_return(df, return_col='daily_return', window=20):\n",
    "    \"\"\"\n",
    "    Create volatility-adjusted return feature.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with return data\n",
    "    return_col : str\n",
    "        Column name for returns\n",
    "    window : int\n",
    "        Rolling window for volatility calculation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Volatility-adjusted returns\n",
    "    \"\"\"\n",
    "    # Calculate rolling volatility\n",
    "    rolling_vol = df.groupby('symbol')[return_col].rolling(window).std().reset_index(0, drop=True)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    rolling_vol = rolling_vol.replace(0, np.nan)\n",
    "    \n",
    "    # Calculate volatility-adjusted return\n",
    "    vol_adj_return = df[return_col] / rolling_vol\n",
    "    \n",
    "    return vol_adj_return\n",
    "\n",
    "# Apply feature engineering\n",
    "if not df.empty:\n",
    "    df['vol_adj_return'] = create_volatility_adjusted_return(df)\n",
    "    \n",
    "    print(\"üìä Feature 1: Volatility-Adjusted Return\")\n",
    "    print(f\"Description: Current return normalized by 20-day rolling volatility\")\n",
    "    print(f\"Range: {df['vol_adj_return'].min():.3f} to {df['vol_adj_return'].max():.3f}\")\n",
    "    print(f\"Mean: {df['vol_adj_return'].mean():.3f}\")\n",
    "    print(f\"Std: {df['vol_adj_return'].std():.3f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Distribution\n",
    "    axes[0].hist(df['vol_adj_return'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_title('Volatility-Adjusted Return Distribution')\n",
    "    axes[0].set_xlabel('Vol-Adjusted Return')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Comparison with raw returns\n",
    "    axes[1].scatter(df['daily_return'], df['vol_adj_return'], alpha=0.5, s=10)\n",
    "    axes[1].set_xlabel('Raw Daily Return')\n",
    "    axes[1].set_ylabel('Volatility-Adjusted Return')\n",
    "    axes[1].set_title('Raw vs Volatility-Adjusted Returns')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature 2: Volume Momentum Indicator\n",
    "\n",
    "**Rationale**: EDA showed strong correlation between volume and price movements. This feature captures volume momentum by comparing current volume to its recent average and combining it with price momentum, indicating institutional interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_volume_momentum(df, volume_col='volume', price_col='close', window=10):\n",
    "    \"\"\"\n",
    "    Create volume momentum indicator.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    volume_col : str\n",
    "        Volume column name\n",
    "    price_col : str\n",
    "        Price column name\n",
    "    window : int\n",
    "        Rolling window for calculations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Volume momentum indicator\n",
    "    \"\"\"\n",
    "    # Volume ratio (current vs average)\n",
    "    volume_ma = df.groupby('symbol')[volume_col].rolling(window).mean().reset_index(0, drop=True)\n",
    "    volume_ratio = df[volume_col] / volume_ma\n",
    "    \n",
    "    # Price momentum (rate of change)\n",
    "    price_momentum = df.groupby('symbol')[price_col].pct_change(window).reset_index(0, drop=True)\n",
    "    \n",
    "    # Combine volume and price momentum\n",
    "    volume_momentum = volume_ratio * np.sign(price_momentum) * np.abs(price_momentum)\n",
    "    \n",
    "    return volume_momentum\n",
    "\n",
    "# Apply feature engineering\n",
    "if not df.empty:\n",
    "    df['volume_momentum'] = create_volume_momentum(df)\n",
    "    \n",
    "    print(\"\\nüìä Feature 2: Volume Momentum Indicator\")\n",
    "    print(f\"Description: Volume ratio weighted by price momentum direction and magnitude\")\n",
    "    print(f\"Range: {df['volume_momentum'].min():.3f} to {df['volume_momentum'].max():.3f}\")\n",
    "    print(f\"Mean: {df['volume_momentum'].mean():.3f}\")\n",
    "    print(f\"Std: {df['volume_momentum'].std():.3f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Distribution\n",
    "    axes[0].hist(df['volume_momentum'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_title('Volume Momentum Distribution')\n",
    "    axes[0].set_xlabel('Volume Momentum')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Relationship with next-day returns\n",
    "    df_temp = df.copy()\n",
    "    df_temp['next_return'] = df_temp.groupby('symbol')['daily_return'].shift(-1)\n",
    "    \n",
    "    momentum_data = df_temp[['volume_momentum', 'next_return']].dropna()\n",
    "    axes[1].scatter(momentum_data['volume_momentum'], momentum_data['next_return'], alpha=0.5, s=10)\n",
    "    axes[1].set_xlabel('Volume Momentum')\n",
    "    axes[1].set_ylabel('Next Day Return')\n",
    "    axes[1].set_title('Volume Momentum vs Future Returns')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature 3: Technical Divergence Signal\n",
    "\n",
    "**Rationale**: Technical analysis suggests that divergences between price and momentum indicators can signal trend reversals. This feature captures when price makes new highs/lows but momentum indicators don't confirm, indicating potential reversal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_technical_divergence(df, price_col='close', window=20):\n",
    "    \"\"\"\n",
    "    Create technical divergence signal.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    price_col : str\n",
    "        Price column name\n",
    "    window : int\n",
    "        Lookback window for divergence calculation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Technical divergence signal\n",
    "    \"\"\"\n",
    "    divergence_signals = []\n",
    "    \n",
    "    for symbol in df['symbol'].unique():\n",
    "        symbol_data = df[df['symbol'] == symbol].copy().sort_values('date')\n",
    "        \n",
    "        # Price momentum (ROC)\n",
    "        price_roc = symbol_data[price_col].pct_change(window)\n",
    "        \n",
    "        # RSI-like momentum indicator\n",
    "        returns = symbol_data[price_col].pct_change()\n",
    "        gains = returns.where(returns > 0, 0)\n",
    "        losses = -returns.where(returns < 0, 0)\n",
    "        \n",
    "        avg_gains = gains.rolling(window).mean()\n",
    "        avg_losses = losses.rolling(window).mean()\n",
    "        \n",
    "        rs = avg_gains / avg_losses\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # Normalize RSI to momentum scale\n",
    "        rsi_momentum = (rsi - 50) / 50  # Convert to -1 to 1 scale\n",
    "        \n",
    "        # Calculate divergence: price momentum vs RSI momentum\n",
    "        divergence = price_roc - rsi_momentum\n",
    "        \n",
    "        # Add symbol identifier and append\n",
    "        divergence_df = pd.DataFrame({\n",
    "            'symbol': symbol,\n",
    "            'date': symbol_data['date'],\n",
    "            'divergence': divergence\n",
    "        })\n",
    "        divergence_signals.append(divergence_df)\n",
    "    \n",
    "    # Combine all symbols\n",
    "    all_divergence = pd.concat(divergence_signals, ignore_index=True)\n",
    "    \n",
    "    # Merge back to original dataframe order\n",
    "    df_with_divergence = df.merge(all_divergence, on=['symbol', 'date'], how='left')\n",
    "    \n",
    "    return df_with_divergence['divergence']\n",
    "\n",
    "# Apply feature engineering\n",
    "if not df.empty:\n",
    "    df['tech_divergence'] = create_technical_divergence(df)\n",
    "    \n",
    "    print(\"\\nüìä Feature 3: Technical Divergence Signal\")\n",
    "    print(f\"Description: Divergence between price momentum and RSI-based momentum\")\n",
    "    print(f\"Range: {df['tech_divergence'].min():.3f} to {df['tech_divergence'].max():.3f}\")\n",
    "    print(f\"Mean: {df['tech_divergence'].mean():.3f}\")\n",
    "    print(f\"Std: {df['tech_divergence'].std():.3f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Distribution\n",
    "    axes[0].hist(df['tech_divergence'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_title('Technical Divergence Distribution')\n",
    "    axes[0].set_xlabel('Divergence Signal')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Time series example for one symbol\n",
    "    sample_symbol = df['symbol'].iloc[0]\n",
    "    sample_data = df[df['symbol'] == sample_symbol].sort_values('date').tail(100)\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    ax2_twin = ax2.twinx()\n",
    "    \n",
    "    ax2.plot(sample_data['date'], sample_data['close'], 'b-', label='Price', alpha=0.7)\n",
    "    ax2_twin.plot(sample_data['date'], sample_data['tech_divergence'], 'r-', label='Divergence', alpha=0.7)\n",
    "    \n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.set_ylabel('Price', color='b')\n",
    "    ax2_twin.set_ylabel('Divergence Signal', color='r')\n",
    "    ax2.set_title(f'Price vs Divergence Signal ({sample_symbol})')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Create target variable (next-day return)\n",
    "    df['target_return'] = df.groupby('symbol')['daily_return'].shift(-1)\n",
    "    \n",
    "    # Select features for correlation analysis\n",
    "    feature_cols = ['vol_adj_return', 'volume_momentum', 'tech_divergence']\n",
    "    target_col = 'target_return'\n",
    "    \n",
    "    # Calculate correlations with target\n",
    "    correlations = []\n",
    "    for feature in feature_cols:\n",
    "        corr = df[feature].corr(df[target_col])\n",
    "        correlations.append({'Feature': feature, 'Correlation': corr})\n",
    "    \n",
    "    corr_df = pd.DataFrame(correlations)\n",
    "    print(\"\\nüîó Feature Correlations with Next-Day Returns:\")\n",
    "    print(corr_df.round(4))\n",
    "    \n",
    "    # Feature correlation matrix\n",
    "    feature_data = df[feature_cols + [target_col]].dropna()\n",
    "    feature_corr_matrix = feature_data.corr()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(feature_corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, fmt='.3f')\n",
    "    plt.title('Engineered Features Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical significance tests\n",
    "    print(\"\\nüìä Statistical Significance Tests:\")\n",
    "    for feature in feature_cols:\n",
    "        feature_data = df[[feature, target_col]].dropna()\n",
    "        if len(feature_data) > 30:  # Minimum sample size\n",
    "            corr, p_value = stats.pearsonr(feature_data[feature], feature_data[target_col])\n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
    "            print(f\"{feature}: r={corr:.4f}, p={p_value:.4f} {significance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Test feature stability across different symbols\n",
    "    print(\"\\nüß™ Feature Stability Analysis:\")\n",
    "    \n",
    "    stability_results = []\n",
    "    for symbol in df['symbol'].unique():\n",
    "        symbol_data = df[df['symbol'] == symbol]\n",
    "        \n",
    "        for feature in feature_cols:\n",
    "            if len(symbol_data) > 50:  # Minimum data points\n",
    "                corr = symbol_data[feature].corr(symbol_data[target_col])\n",
    "                stability_results.append({\n",
    "                    'Symbol': symbol,\n",
    "                    'Feature': feature,\n",
    "                    'Correlation': corr\n",
    "                })\n",
    "    \n",
    "    stability_df = pd.DataFrame(stability_results)\n",
    "    \n",
    "    # Pivot for better visualization\n",
    "    stability_pivot = stability_df.pivot(index='Symbol', columns='Feature', values='Correlation')\n",
    "    print(stability_pivot.round(4))\n",
    "    \n",
    "    # Feature importance ranking\n",
    "    print(\"\\nüèÜ Feature Importance Ranking:\")\n",
    "    feature_importance = stability_df.groupby('Feature')['Correlation'].agg([\n",
    "        'mean', 'std', 'count'\n",
    "    ]).round(4)\n",
    "    feature_importance['abs_mean'] = feature_importance['mean'].abs()\n",
    "    feature_importance = feature_importance.sort_values('abs_mean', ascending=False)\n",
    "    \n",
    "    print(feature_importance)\n",
    "    \n",
    "    # Visualize feature performance across symbols\n",
    "    fig, axes = plt.subplots(1, len(feature_cols), figsize=(15, 5))\n",
    "    \n",
    "    for i, feature in enumerate(feature_cols):\n",
    "        feature_data = stability_df[stability_df['Feature'] == feature]\n",
    "        axes[i].bar(feature_data['Symbol'], feature_data['Correlation'], alpha=0.7)\n",
    "        axes[i].set_title(f'{feature}\\nCorrelation by Symbol')\n",
    "        axes[i].set_ylabel('Correlation')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        axes[i].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering Summary\n",
    "\n",
    "### Implemented Features\n",
    "\n",
    "**1. Volatility-Adjusted Return (`vol_adj_return`)**\n",
    "- **Formula**: `daily_return / rolling_volatility_20d`\n",
    "- **Rationale**: Normalizes returns by recent volatility to capture risk-adjusted performance\n",
    "- **Use Case**: Identifies periods of unusual return magnitude relative to recent volatility\n",
    "\n",
    "**2. Volume Momentum Indicator (`volume_momentum`)**\n",
    "- **Formula**: `(volume / volume_ma_10d) * sign(price_momentum) * abs(price_momentum)`\n",
    "- **Rationale**: Combines volume activity with price momentum direction and magnitude\n",
    "- **Use Case**: Captures institutional interest and conviction behind price moves\n",
    "\n",
    "**3. Technical Divergence Signal (`tech_divergence`)**\n",
    "- **Formula**: `price_momentum_20d - rsi_momentum_normalized`\n",
    "- **Rationale**: Identifies divergences between price and momentum indicators\n",
    "- **Use Case**: Signals potential trend reversals when price and momentum diverge\n",
    "\n",
    "### Feature Performance\n",
    "\n",
    "Based on correlation analysis with next-day returns:\n",
    "- Features show varying predictive power across different symbols\n",
    "- Statistical significance varies, indicating feature effectiveness depends on market conditions\n",
    "- Feature stability analysis reveals which features are more robust across different stocks\n",
    "\n",
    "### Connection to EDA Insights\n",
    "\n",
    "1. **Volume-Return Relationship**: Volume momentum feature directly addresses the strong correlation observed in EDA\n",
    "2. **Non-Normal Returns**: Volatility-adjusted returns help normalize the fat-tailed distribution\n",
    "3. **Technical Patterns**: Divergence signal captures the mean-reversion tendencies identified in EDA\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Feature Selection**: Use statistical tests and cross-validation to select most predictive features\n",
    "2. **Feature Scaling**: Standardize features for machine learning models\n",
    "3. **Interaction Terms**: Consider feature interactions and polynomial terms\n",
    "4. **Time-Based Features**: Add seasonal and cyclical components identified in EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Save dataset with engineered features\n",
    "    feature_dataset = df[['symbol', 'date', 'close', 'volume', 'daily_return'] + feature_cols + ['target_return']].copy()\n",
    "    \n",
    "    # Save to file\n",
    "    output_path = utils.save_with_timestamp(\n",
    "        df=feature_dataset,\n",
    "        prefix=\"engineered_features\",\n",
    "        source=\"homework\",\n",
    "        ext=\"csv\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüíæ Engineered features saved to: {output_path}\")\n",
    "    \n",
    "    print(\"\\nüéØ Feature Engineering Summary:\")\n",
    "    print(f\"‚úÖ {len(feature_cols)} new features created\")\n",
    "    print(f\"‚úÖ Features tested for correlation with target\")\n",
    "    print(f\"‚úÖ Stability analysis across {df['symbol'].nunique()} symbols\")\n",
    "    print(f\"‚úÖ Dataset ready for modeling: {feature_dataset.shape}\")\n",
    "    \n",
    "    print(\"\\nüìä Final Feature Set:\")\n",
    "    for i, feature in enumerate(feature_cols, 1):\n",
    "        print(f\"{i}. {feature}: {feature_dataset[feature].describe().round(4).to_dict()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
