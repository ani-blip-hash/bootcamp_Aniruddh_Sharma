{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 10b: Time Series Modeling for Bike Demand Prediction\n",
    "\n",
    "## Overview\n",
    "Time series modeling approach with sklearn Pipeline, lag/rolling features, and temporal validation.\n",
    "\n",
    "## Modeling Approach: Time Series\n",
    "**Rationale**: Bike demand exhibits strong temporal patterns with hourly seasonality, making time series modeling optimal for capturing sequential dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "print(\"ðŸ“ˆ Time Series Libraries Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare time series data\n",
    "data = pd.read_csv('../data/sample-data.csv')\n",
    "data['datetime'] = pd.to_datetime('2024-01-01') + pd.to_timedelta(data.index, unit='h')\n",
    "data = data.set_index('datetime')\n",
    "\n",
    "print(f\"ðŸ“Š Time Series Dataset: {data.shape}\")\n",
    "print(f\"ðŸ“… Range: {data.index.min()} to {data.index.max()}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Feature Engineering with sklearn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer for lag and rolling features\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TimeSeriesFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target_col='demand', lags=[1, 2, 3, 6], windows=[3, 6, 12]):\n",
    "        self.target_col = target_col\n",
    "        self.lags = lags\n",
    "        self.windows = windows\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        # Lag features\n",
    "        for lag in self.lags:\n",
    "            X_new[f'{self.target_col}_lag_{lag}'] = X_new[self.target_col].shift(lag)\n",
    "        \n",
    "        # Rolling features\n",
    "        for window in self.windows:\n",
    "            X_new[f'{self.target_col}_rolling_{window}h'] = X_new[self.target_col].rolling(window, min_periods=1).mean()\n",
    "            X_new[f'temp_rolling_{window}h'] = X_new['temperature'].rolling(window, min_periods=1).mean()\n",
    "        \n",
    "        # Temporal features\n",
    "        X_new['hour'] = X_new.index.hour\n",
    "        X_new['hour_sin'] = np.sin(2 * np.pi * X_new['hour'] / 24)\n",
    "        X_new['hour_cos'] = np.cos(2 * np.pi * X_new['hour'] / 24)\n",
    "        X_new['is_weekend'] = (X_new.index.dayofweek >= 5).astype(int)\n",
    "        \n",
    "        # Differencing\n",
    "        X_new['demand_diff'] = X_new[self.target_col].diff()\n",
    "        X_new['temp_diff'] = X_new['temperature'].diff()\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "# Apply feature engineering\n",
    "ts_transformer = TimeSeriesFeatureTransformer()\n",
    "data_transformed = ts_transformer.transform(data)\n",
    "data_clean = data_transformed.dropna()\n",
    "\n",
    "print(f\"ðŸ”§ Feature Engineering Complete:\")\n",
    "print(f\"   Original: {data.shape[1]} features\")\n",
    "print(f\"   Transformed: {data_transformed.shape[1]} features\")\n",
    "print(f\"   After cleaning: {data_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "target_col = 'demand'\n",
    "feature_cols = [col for col in data_clean.columns if col != target_col]\n",
    "X = data_clean[feature_cols]\n",
    "y = data_clean[target_col]\n",
    "\n",
    "# Time series split (80/20)\n",
    "split_idx = int(0.8 * len(data_clean))\n",
    "X_train = X.iloc[:split_idx]\n",
    "X_test = X.iloc[split_idx:]\n",
    "y_train = y.iloc[:split_idx]\n",
    "y_test = y.iloc[split_idx:]\n",
    "\n",
    "print(f\"ðŸ“ˆ Time Series Split:\")\n",
    "print(f\"   Train: {len(X_train)} samples ({len(X_train)/len(data_clean)*100:.1f}%)\")\n",
    "print(f\"   Test: {len(X_test)} samples ({len(X_test)/len(data_clean)*100:.1f}%)\")\n",
    "print(f\"   No temporal overlap: {X_train.index.max() < X_test.index.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. sklearn Pipeline with Multiple Model Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for time series\n",
    "correlations = X_train.corrwith(y_train).abs().sort_values(ascending=False)\n",
    "top_features = correlations.head(12).index.tolist()\n",
    "\n",
    "print(f\"ðŸ† Top 12 Features Selected:\")\n",
    "for i, feature in enumerate(top_features, 1):\n",
    "    corr = correlations[feature]\n",
    "    print(f\"   {i:2d}. {feature:25s} (r={corr:.3f})\")\n",
    "\n",
    "X_train_sel = X_train[top_features]\n",
    "X_test_sel = X_test[top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sklearn Pipelines\n",
    "pipelines = {\n",
    "    'Linear_TS': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LinearRegression())\n",
    "    ]),\n",
    "    'Ridge_TS': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Ridge(alpha=1.0, random_state=42))\n",
    "    ]),\n",
    "    'Ridge_Strong': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Ridge(alpha=10.0, random_state=42))\n",
    "    ]),\n",
    "    'RandomForest_TS': Pipeline([\n",
    "        ('model', RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "results = {}\n",
    "print(\"ðŸ¤– Training Time Series Pipelines:\")\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    pipeline.fit(X_train_sel, y_train)\n",
    "    \n",
    "    y_train_pred = pipeline.predict(X_train_sel)\n",
    "    y_test_pred = pipeline.predict(X_test_sel)\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_r2': r2_score(y_train, y_train_pred),\n",
    "        'test_r2': r2_score(y_test, y_test_pred),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'test_mae': mean_absolute_error(y_test, y_test_pred),\n",
    "        'predictions': y_test_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"   {name}: RÂ²={results[name]['test_r2']:.3f}, RMSE={results[name]['test_rmse']:.2f}\")\n",
    "\n",
    "best_model = max(results.keys(), key=lambda k: results[k]['test_r2'])\n",
    "print(f\"\\nðŸ† Best Model: {best_model} (RÂ²={results[best_model]['test_r2']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Series Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesSplit cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "X_full = pd.concat([X_train_sel, X_test_sel])\n",
    "y_full = pd.concat([y_train, y_test])\n",
    "\n",
    "cv_scores = []\n",
    "best_pipeline = pipelines[best_model]\n",
    "\n",
    "print(f\"ðŸ“Š Time Series Cross-Validation ({best_model}):\")\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X_full), 1):\n",
    "    X_cv_train = X_full.iloc[train_idx]\n",
    "    X_cv_test = X_full.iloc[test_idx]\n",
    "    y_cv_train = y_full.iloc[train_idx]\n",
    "    y_cv_test = y_full.iloc[test_idx]\n",
    "    \n",
    "    cv_pipeline = Pipeline(best_pipeline.steps)\n",
    "    cv_pipeline.fit(X_cv_train, y_cv_train)\n",
    "    y_cv_pred = cv_pipeline.predict(X_cv_test)\n",
    "    \n",
    "    cv_score = r2_score(y_cv_test, y_cv_pred)\n",
    "    cv_scores.append(cv_score)\n",
    "    print(f\"   Fold {fold}: RÂ² = {cv_score:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ CV Results: {np.mean(cv_scores):.3f} Â± {np.std(cv_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Diagnostic Plots and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series diagnostics\n",
    "best_predictions = results[best_model]['predictions']\n",
    "residuals = y_test - best_predictions\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Actual vs Predicted time series\n",
    "axes[0,0].plot(y_test.index, y_test.values, label='Actual', linewidth=2)\n",
    "axes[0,0].plot(y_test.index, best_predictions, label='Predicted', linewidth=2)\n",
    "axes[0,0].set_title('Time Series Forecast')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals over time\n",
    "axes[0,1].plot(y_test.index, residuals)\n",
    "axes[0,1].axhline(y=0, color='red', linestyle='--')\n",
    "axes[0,1].set_title('Residuals Over Time')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals distribution\n",
    "axes[1,0].hist(residuals, bins=12, alpha=0.7, edgecolor='black')\n",
    "axes[1,0].axvline(residuals.mean(), color='red', linestyle='--')\n",
    "axes[1,0].set_title('Residuals Distribution')\n",
    "\n",
    "# Actual vs Predicted scatter\n",
    "axes[1,1].scatter(y_test, best_predictions, alpha=0.7)\n",
    "axes[1,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "axes[1,1].set_title('Actual vs Predicted')\n",
    "axes[1,1].set_xlabel('Actual')\n",
    "axes[1,1].set_ylabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ðŸ“Š Residual Analysis:\")\n",
    "print(f\"   Mean: {residuals.mean():.6f}\")\n",
    "print(f\"   Std: {residuals.std():.3f}\")\n",
    "print(f\"   Skewness: {residuals.skew():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Performance Summary and Risk Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary\n",
    "performance_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Test_R2': [results[m]['test_r2'] for m in results.keys()],\n",
    "    'Test_RMSE': [results[m]['test_rmse'] for m in results.keys()],\n",
    "    'Test_MAE': [results[m]['test_mae'] for m in results.keys()],\n",
    "    'Overfitting': [results[m]['train_r2'] - results[m]['test_r2'] for m in results.keys()]\n",
    "}).sort_values('Test_R2', ascending=False)\n",
    "\n",
    "print(\"ðŸ“Š Time Series Model Performance:\")\n",
    "print(performance_df.round(3))\n",
    "\n",
    "# Risk assessment\n",
    "best_r2 = results[best_model]['test_r2']\n",
    "best_rmse = results[best_model]['test_rmse']\n",
    "best_mae = results[best_model]['test_mae']\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Time Series Model Insights:\")\n",
    "print(f\"   â€¢ Explains {best_r2:.1%} of demand variance\")\n",
    "print(f\"   â€¢ Average forecast error: {best_rmse:.1f} bikes\")\n",
    "print(f\"   â€¢ Mean absolute error: {best_mae:.1f} bikes\")\n",
    "print(f\"   â€¢ Cross-validation stability: {np.std(cv_scores):.3f}\")\n",
    "\n",
    "print(f\"\\nâš ï¸ Time Series Risk Assessment:\")\n",
    "max_error = abs(residuals).max()\n",
    "print(f\"   â€¢ Maximum forecast error: {max_error:.1f} bikes\")\n",
    "print(f\"   â€¢ 95% of errors within: Â±{np.percentile(abs(residuals), 95):.1f} bikes\")\n",
    "print(f\"   â€¢ Temporal dependencies: {'Captured' if best_r2 > 0.7 else 'Partially captured'}\")\n",
    "print(f\"   â€¢ Model stability: {'High' if np.std(cv_scores) < 0.1 else 'Moderate'}\")\n",
    "\n",
    "# Business interpretation\n",
    "demand_range = y.max() - y.min()\n",
    "relative_error = best_rmse / demand_range\n",
    "print(f\"   â€¢ Relative error: {relative_error:.1%} of demand range\")\n",
    "print(f\"   â€¢ Suitable for: {'Real-time forecasting' if relative_error < 0.15 else 'Strategic planning'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "trained_pipeline = pipelines[best_model]\n",
    "\n",
    "if hasattr(trained_pipeline.named_steps['model'], 'coef_'):\n",
    "    # Linear model coefficients\n",
    "    coefficients = pd.DataFrame({\n",
    "        'feature': top_features,\n",
    "        'coefficient': trained_pipeline.named_steps['model'].coef_\n",
    "    }).sort_values('coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    print(f\"ðŸ” {best_model} Feature Coefficients:\")\n",
    "    print(coefficients.head(8))\n",
    "    \n",
    "elif hasattr(trained_pipeline.named_steps['model'], 'feature_importances_'):\n",
    "    # Tree-based importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': top_features,\n",
    "        'importance': trained_pipeline.named_steps['model'].feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"ðŸŒ³ {best_model} Feature Importance:\")\n",
    "    print(importance.head(8))\n",
    "\n",
    "# Time series specific insights\n",
    "lag_features = [f for f in top_features if 'lag' in f]\n",
    "rolling_features = [f for f in top_features if 'rolling' in f]\n",
    "temporal_features = [f for f in top_features if any(x in f for x in ['hour', 'weekend'])]\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Time Series Feature Categories:\")\n",
    "print(f\"   â€¢ Lag features: {len(lag_features)} ({lag_features})\")\n",
    "print(f\"   â€¢ Rolling features: {len(rolling_features)}\")\n",
    "print(f\"   â€¢ Temporal features: {len(temporal_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save time series model results\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save predictions with timestamps\n",
    "ts_predictions = pd.DataFrame({\n",
    "    'datetime': y_test.index,\n",
    "    'actual': y_test.values,\n",
    "    'predicted': best_predictions,\n",
    "    'residuals': residuals\n",
    "})\n",
    "ts_predictions.to_csv('../data/processed/time_series_predictions.csv', index=False)\n",
    "\n",
    "# Save performance summary\n",
    "performance_df.to_csv('../data/processed/time_series_performance.csv', index=False)\n",
    "\n",
    "print(\"ðŸ’¾ Time Series Results Saved:\")\n",
    "print(\"   â€¢ Predictions: ../data/processed/time_series_predictions.csv\")\n",
    "print(\"   â€¢ Performance: ../data/processed/time_series_performance.csv\")\n",
    "print(f\"\\nðŸŽ¯ Time Series Modeling Complete!\")\n",
    "print(f\"   Best model: {best_model} (RÂ²={best_r2:.3f})\")\n",
    "print(f\"   Forecast accuracy: {best_rmse:.1f} bikes RMSE\")\n",
    "print(f\"   Cross-validation: {np.mean(cv_scores):.3f} Â± {np.std(cv_scores):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
